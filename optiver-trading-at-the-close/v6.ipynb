{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d47cbad",
   "metadata": {},
   "source": [
    "Optimized XGBRegressor got us to 8.4405, let's see what LightGBM can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3fa3187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import typing as t\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# %load_ext cudf.pandas\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA = './.data/train.csv' # !!update for Kaggle!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5e98503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data:pd.DataFrame) -> pd.DataFrame:\n",
    "    assert 'target' not in data.columns # sanity check\n",
    "    data = data.reset_index().set_index('row_id')\n",
    "    for drop_col in ['index', 'time_id', 'currently_scored']:\n",
    "        if drop_col in data.columns:\n",
    "            data = data.drop(drop_col, axis=1)\n",
    "    id_cols = ['date_id', 'seconds_in_bucket', 'stock_id']\n",
    "    data = data.sort_values(by=id_cols)\n",
    "    data = data.drop(id_cols, axis=1)\n",
    "    data = (data - data.min()) / (data.max() - data.min())\n",
    "    data = data.ffill().fillna(0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e32ba6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_vars(path:str=TRAINING_DATA) -> tuple[pd.DataFrame, pd.Series]:\n",
    "    data = pd.read_csv(path, index_col='row_id')\n",
    "    data = data.dropna(subset=['target'])\n",
    "    y = data.target\n",
    "    X = data.drop('target', axis=1)\n",
    "    X = preprocess(X)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c0a6b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(t.Protocol):\n",
    "    def fit(self, X, y, sample_weight=None): ...\n",
    "    def predict(self, X): ...\n",
    "\n",
    "def train_model(model:type|Model, model_kw:dict={}, fold_kw:dict={}) -> Model:\n",
    "    print(f'Pre-training...', end='\\r')\n",
    "    gc.collect() # just in case\n",
    "    X, y = load_training_vars()    \n",
    "    cv = TimeSeriesSplit(**fold_kw)\n",
    "    folds = cv.get_n_splits()\n",
    "    if isinstance(model, type):\n",
    "        model = model(**model_kw)\n",
    "    print(f'Pre-training - Done.')\n",
    "    # early_stop = 'early_stopping_roungs' in model_kwargs\n",
    "    for i, (i_train, i_valid) in enumerate(cv.split(X)):\n",
    "        print(f'Training: Fold {i + 1}/{folds} - Running...', end='\\r')\n",
    "        try:\n",
    "            model.fit(X.iloc[i_train, :], y[i_train], verbose=False) #, eval_set=[(X_valid, y_valid)] if early_stop else None)\n",
    "            mae = mean_absolute_error(y[i_valid], model.predict(X.iloc[i_valid, :]))\n",
    "        except Exception as e:\n",
    "            print(f'Training: Fold {i + 1}/{folds} - Failed: {e}')\n",
    "            print(f'Returning undertrained model ({i} folds)')\n",
    "            break\n",
    "        print(f'Training: Fold {i + 1}/{folds} - Done. MAE: {mae}')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training - Done.\n",
      "Training: Fold 1/5 - Done. MAE: 7.365128684211296\n",
      "Training: Fold 2/5 - Done. MAE: 6.834012097098109\n",
      "Training: Fold 3/5 - Done. MAE: 6.13850643808549\n",
      "Training: Fold 4/5 - Done. MAE: 6.370388844816867\n",
      "Training: Fold 5/5 - Done. MAE: 5.936786897874561\n"
     ]
    }
   ],
   "source": [
    "train_model(lgb.LGBMRegressor)\n",
    "pass # lgb baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_params = dict(seed=25, n_jobs=16, learning_rate=0.2, max_depth=3, colsample_bytree=0.85, subsample=0.8, reg_alpha=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training - Done.\n",
      "Training: Fold 1/5 - Done. MAE: 7.345823785360344\n",
      "Training: Fold 2/5 - Done. MAE: 6.827276161701868\n",
      "Training: Fold 3/5 - Done. MAE: 6.142414891861338\n",
      "Training: Fold 4/5 - Done. MAE: 6.370331139121156\n",
      "Training: Fold 5/5 - Done. MAE: 5.934155244654084\n"
     ]
    }
   ],
   "source": [
    "xgb_params = dict(tree_method='hist', eval_metric='mae', gamma=0.2, **shared_params)\n",
    "train_model(xgb.XGBRegressor, xgb_params)\n",
    "pass # tuned xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training - Done.\n",
      "Training: Fold 1/5 - Done. MAE: 7.3418280122986275\n",
      "Training: Fold 2/5 - Done. MAE: 6.828364364682563\n",
      "Training: Fold 3/5 - Done. MAE: 6.140877500590833\n",
      "Training: Fold 4/5 - Done. MAE: 6.369030754339805\n",
      "Training: Fold 5/5 - Done. MAE: 5.9343541090902105\n"
     ]
    }
   ],
   "source": [
    "lgb_params = dict(metric='l1', **shared_params)\n",
    "train_model(lgb.LGBMRegressor, shared_params)\n",
    "pass # compare to tuned xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(model:Model, param_grid:dict, n_jobs:int=8) -> Model:\n",
    "    print('Starting grid search...', end='\\r')\n",
    "    X, y = load_training_vars()\n",
    "    search = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=n_jobs)\n",
    "    search.fit(X, y)\n",
    "    print(f'Grid search complete. Best params: {search.best_params_}')\n",
    "    return search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search complete. Best params: {'max_leaves': 2}\n",
      "Pre-training - Done.\n",
      "Training: Fold 1/5 - Done. MAE: 7.380403400253767\n",
      "Training: Fold 2/5 - Done. MAE: 6.855299064685574\n",
      "Training: Fold 3/5 - Done. MAE: 6.154777026210802\n",
      "Training: Fold 4/5 - Done. MAE: 6.383582022118538\n",
      "Training: Fold 5/5 - Done. MAE: 5.947080130939923\n"
     ]
    }
   ],
   "source": [
    "model = xgb.XGBRegressor(**xgb_params)\n",
    "model = grid_search(model, param_grid={'max_leaves':[2**x for x in range(1, 5)]}) # best was 2\n",
    "model = train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search complete. Best params: {'num_leaves': 16}\n",
      "Pre-training - Done.\n",
      "Training: Fold 1/5 - Done. MAE: 7.3418280122986275\n",
      "Training: Fold 2/5 - Done. MAE: 6.828364364682563\n",
      "Training: Fold 3/5 - Done. MAE: 6.140877500590833\n",
      "Training: Fold 4/5 - Done. MAE: 6.369030754339805\n",
      "Training: Fold 5/5 - Done. MAE: 5.9343541090902105\n"
     ]
    }
   ],
   "source": [
    "model = lgb.LGBMRegressor(**lgb_params)\n",
    "model = grid_search(model, param_grid={'num_leaves':[2**x for x in range(1, 5)]}) # best = 8\n",
    "model = train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb_params = dict(metric='l1', num_leaves=8, **shared_params)\n",
    "# model = lgb.LGBMRegressor(**lgb_params)\n",
    "# model = grid_search(model, param_grid={'min_data_in_leaf':[10**x for x in range(2, 5)], # these are overriding aliases used by the python API\n",
    "#                                        'min_gain_to_split': [0, 0.1, 1, 10]}) # https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html\n",
    "# model = train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search complete. Best params: {'min_child_samples': 2000, 'min_split_gain': 0.01}\n",
      "Pre-training - Done.\n",
      "Training: Fold 1/5 - Done. MAE: 7.341038789452177\n",
      "Training: Fold 2/5 - Done. MAE: 6.8271654582570305\n",
      "Training: Fold 3/5 - Done. MAE: 6.142087789705268\n",
      "Training: Fold 4/5 - Done. MAE: 6.369221848953748\n",
      "Training: Fold 5/5 - Done. MAE: 5.932219891077756\n"
     ]
    }
   ],
   "source": [
    "lgb_params = dict(metric='l1', num_leaves=8, **shared_params)\n",
    "model = lgb.LGBMRegressor(**lgb_params)\n",
    "model = grid_search(model, param_grid={'min_child_samples':[2 * 10**x for x in range(2, 5)],\n",
    "                                       'min_split_gain': [0, 0.01, 0.05, 0.1, 0.5]})\n",
    "model = train_model(model) # 5.932219891077756 (first improvement over v5!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search complete. Best params: {'min_child_samples': 2000, 'min_split_gain': 0.001}\n",
      "Pre-training - Done.\n",
      "Training: Fold 1/5 - Done. MAE: 7.341038789452177\n",
      "Training: Fold 2/5 - Done. MAE: 6.8271654582570305\n",
      "Training: Fold 3/5 - Done. MAE: 6.142087789705268\n",
      "Training: Fold 4/5 - Done. MAE: 6.369221848953748\n",
      "Training: Fold 5/5 - Done. MAE: 5.932219891077756\n"
     ]
    }
   ],
   "source": [
    "# refine prior results\n",
    "model = lgb.LGBMRegressor(**lgb_params)\n",
    "model = grid_search(model, param_grid={'min_child_samples':[500, 1000, 2000, 4000, 8000], # last run was middle of the range (2000) -> confirmed\n",
    "                                       'min_split_gain': [0.001, 0.005, 0.01, 0.02]}) # similar result (0.01) -> 0.001\n",
    "model = train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training - Done.\n",
      "Training: Fold 1/5 - Done. MAE: 7.341038789452177\n",
      "Training: Fold 2/5 - Done. MAE: 6.8271654582570305\n",
      "Training: Fold 3/5 - Done. MAE: 6.142087789705268\n",
      "Training: Fold 4/5 - Done. MAE: 6.369221848953748\n",
      "Training: Fold 5/5 - Done. MAE: 5.932219891077756\n"
     ]
    }
   ],
   "source": [
    "lgb_params = dict(metric='l1', num_leaves=8, min_child_samples=2000, min_split_gain=0.001, **shared_params)\n",
    "model = lgb.LGBMRegressor(**lgb_params)\n",
    "model = train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22e14226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission compat check\n",
    "import optiver2023\n",
    "env = optiver2023.make_env()\n",
    "iter_test = env.iter_test()\n",
    "\n",
    "for (test, revealed_targets, _) in iter_test:\n",
    "    X_test = preprocess(test)\n",
    "    y_pred = model.predict(X_test)\n",
    "    submission = test[['row_id']]\n",
    "    submission['target'] = y_pred\n",
    "    env.predict(submission)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
