{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another rewrite because I had to rebuild my local environment for cudf + tensorflow to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!---------------!UPDATE!---------------!!\n",
    "DATA_TRAIN = '.data/train.csv'\n",
    "DATA_TEST_X = '.data/test.csv'\n",
    "DATA_TEST_Y = '.data/revealed_targets.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # https://stackoverflow.com/questions/40426502/is-there-a-way-to-suppress-the-messages-tensorflow-prints\n",
    "import gc\n",
    "import sys\n",
    "import typing as t\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "try: # got tired of changing code between local and kaggle setup\n",
    "    import cudf.pandas\n",
    "    cudf.pandas.install() # must be called before pandas import\n",
    "except ModuleNotFoundError:\n",
    "    print('cudf not installed. Continuing in CPU mode.')\n",
    "    from catboost import CatBoostRegressor # CatBoost doesn't support cudf so we exclude it locally\n",
    "import pandas as pd\n",
    "import tensorflow as tf # https://github.com/tensorflow/tensorflow/issues/62075\n",
    "keras = tf.keras # https://github.com/microsoft/pylance-release/issues/1066\n",
    "from keras import Sequential, layers\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROPS = ['index', 'time_id', 'currently_scored', 'time_id_x', 'time_id_y', 'revealed_date_id', 'revealed_time_id']\n",
    "SORTS = ['date_id', 'seconds_in_bucket', 'stock_id']\n",
    "INDEX = 'row_id'\n",
    "\n",
    "def preprocess(data:pd.DataFrame, ycol:str=None) -> pd.DataFrame: # separate for submission compat\n",
    "    data = data.reset_index().set_index(INDEX)\n",
    "    data = data.drop([col for col in DROPS if col in data.columns], axis=1)\n",
    "    data = data.sort_values(by=SORTS).drop(SORTS, axis=1)\n",
    "    data = (data - data.min()) / (data.max() - data.min())\n",
    "    data = data.ffill().fillna(0)\n",
    "    return data\n",
    "\n",
    "def load_vars(testing:bool=False) -> tuple[pd.DataFrame, pd.Series]:\n",
    "    if testing:\n",
    "        data = pd.merge(*[pd.read_csv(path) for path in [DATA_TEST_X, DATA_TEST_Y]], on=SORTS) # https://stackoverflow.com/a/32041277/3178898\n",
    "        ycol = 'revealed_target'\n",
    "    else:\n",
    "        data = pd.read_csv(DATA_TRAIN, index_col=INDEX)\n",
    "        ycol = 'target'\n",
    "    data = data.dropna(subset=[ycol]) # some targets are null\n",
    "    return preprocess(data.drop(ycol, axis=1)), data[ycol]\n",
    "\n",
    "N_FEATURES = 11 # update if/as features are engineered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(t.Protocol):\n",
    "    def fit(self, X, y, sample_weight=None): ...\n",
    "    def predict(self, X): ...\n",
    "    def get_params(self, deep=True): ...\n",
    "\n",
    "class ModelProfile:\n",
    "    def __init__(self, model:Model, score:float) -> None:\n",
    "        self.model = model\n",
    "        self.score = score\n",
    "\n",
    "class Ensemble:\n",
    "    def __init__(self) -> None:\n",
    "        self.models = list[ModelProfile]()\n",
    "\n",
    "    @property\n",
    "    def best_score(self) -> float:\n",
    "        return min(m.score for m in self.models) if len(self) > 0 else None\n",
    "    \n",
    "    @property\n",
    "    def mean_score(self) -> float:\n",
    "        return sum(m.score for m in self.models) / len(self) if len(self) > 0 else None\n",
    "\n",
    "    def add(self, model: ModelProfile) -> bool:\n",
    "        if self.mean_score is not None and model.score > self.mean_score:\n",
    "            return False\n",
    "        self.models.append(model)\n",
    "        return True\n",
    "        \n",
    "    def prune(self) -> int:\n",
    "        self.models = [m for m in self.models if m.score < self.mean_score]\n",
    "        return len(self)\n",
    "    \n",
    "    def predict(self, X:pd.DataFrame) -> pd.DataFrame:\n",
    "        y = pd.DataFrame(index=X.index)\n",
    "        y['pred'] = 0\n",
    "        for model in self.models:\n",
    "            y.pred += model.model.predict(X)\n",
    "        y.pred = y.pred / len(self)\n",
    "        return y\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.models)\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f'<Ensemble ({len(self)} model(s); mean_score={self.mean_score}, best_score={self.best_score})>'\n",
    "\n",
    "# accepts a list of Models and/or (class, params) pairs and returns an ensemble of the best performers\n",
    "# an existing Ensemble can also be passed in to update it.\n",
    "def train_ensemble(models:list[Model|tuple[type, dict]], folds:int=5, ens:Ensemble=Ensemble()) -> Ensemble:\n",
    "\n",
    "    print(f'Pre-training setup...', end='\\r')\n",
    "    cv = TimeSeriesSplit(folds)\n",
    "    X, y = load_vars()\n",
    "    X_test, y_test = load_vars(testing=True)\n",
    "    \n",
    "    for model in models:\n",
    "        \n",
    "        # instantiate (if not already)\n",
    "        if isinstance(model, tuple):\n",
    "            model = Model(model[0](**model[1]))\n",
    "        model_class = type(model).__name__\n",
    "        \n",
    "        # customize fit() and predict() kwargs for each model type\n",
    "        fit_kw = dict()\n",
    "        predict_kw = dict()\n",
    "        match model_class:\n",
    "            case 'Sequential':\n",
    "                model.compile(optimizer='adam', loss='mae')\n",
    "                keras_kw = dict(batch_size=256, verbose=0)\n",
    "                fit_kw.update(keras_kw)\n",
    "                predict_kw.update(keras_kw)\n",
    "            case 'LGBMRegressor':\n",
    "                pass\n",
    "            case 'XGBRegressor':\n",
    "                fit_kw.update(dict(verbose=0))\n",
    "            case 'CatBoost':\n",
    "                fit_kw.update(dict(verbose=False))\n",
    "        \n",
    "        # early_stop = any(x in model.get_params() for x in ['early_stopping_rounds', 'early_stopping_round'])\n",
    "        \n",
    "        # k-fold cross-validation\n",
    "        for i, (train, valid) in enumerate(cv.split(X)):\n",
    "            try: # sometimes a training round can fail, but I don't want to give up on the whole ensemble\n",
    "                \n",
    "                print(f'Training {model_class}: Fold {i + 1}/{folds} - Running...', end='\\r')\n",
    "                model.fit(X.iloc[train, :], y[train], **fit_kw)\n",
    "                print(f'Training {model_class}: Fold {i + 1}/{folds} - Complete.  ')\n",
    "\n",
    "                mae_train = mean_absolute_error(y[valid], model.predict(X.iloc[valid, :], **predict_kw))\n",
    "                mae_test = mean_absolute_error(y_test, model.predict(X_test, **predict_kw))\n",
    "                print(f'\\tTrain MAE:  {mae_train}\\n\\tTest MAE:   {mae_test}')\n",
    "\n",
    "                if ens.add(ModelProfile(model, mae_test)):\n",
    "                    print(f'Model accepted.')\n",
    "                else:\n",
    "                    print(f'Model rejected.')\n",
    "                print(f'\\tMean score: {ens.mean_score}\\n\\tBest score: {ens.best_score}')\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f'Training {model_class}: Fold {i + 1}/{folds} - Error: {e}')\n",
    "                break # if a round fails it's usually a model misconfig, so move on to the next model\n",
    "            \n",
    "            finally:\n",
    "                gc.collect() # local setup runs out of memory\n",
    "    \n",
    "    return ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LGBMRegressor: Fold 1/2 - Complete.  \n",
      "\tTrain MAE:  6.49258150865267\n",
      "\tTest MAE:   12.413096267303516\n",
      "Model accepted.\n",
      "\tMean score: 12.413096267303516\n",
      "\tBest score: 12.413096267303516\n",
      "Training LGBMRegressor: Fold 2/2 - Complete.  \n",
      "\tTrain MAE:  6.155308119798974\n",
      "\tTest MAE:   9.234917407387922\n",
      "Model accepted.\n",
      "\tMean score: 10.824006837345719\n",
      "\tBest score: 9.234917407387922\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    # Sequential([\n",
    "    #     layers.Dense(units=1, input_shape=[N_FEATURES])\n",
    "    # ]),\n",
    "    LGBMRegressor(verbosity=-1),\n",
    "    # XGBRegressor(n_jobs=16),\n",
    "]\n",
    "if 'catboost' in sys.modules:\n",
    "    models.append(CatBoostRegressor(silent=True)) # see imports\n",
    "\n",
    "ens = train_ensemble(models, folds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Ensemble (1 model(s); mean_score=9.234917407387922, best_score=9.234917407387922)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ens.prune()\n",
    "ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.234009679922847"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_model(model:Model|Ensemble) -> float:\n",
    "    X_test, y_test = load_vars(testing=True)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "test_model(ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission compat check\n",
    "import optiver2023\n",
    "env = optiver2023.make_env()\n",
    "iter_test = env.iter_test()\n",
    "\n",
    "for (test, revealed_targets, _) in iter_test:\n",
    "    X_test = preprocess(test)\n",
    "    y_pred = ens.predict(X_test)\n",
    "    submission = test[['row_id']].set_index('row_id') # needed to match rows\n",
    "    submission['target'] = y_pred\n",
    "    submission = submission.reset_index() # convert back for final CSV write\n",
    "    env.predict(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRegressor.feature_importances_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
