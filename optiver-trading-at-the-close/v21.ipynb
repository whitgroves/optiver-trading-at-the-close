{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Largely built on [@iqmansingh's](https://www.kaggle.com/iqmansingh) notebook, [4-Fold Time-Series Split Ensemble](https://www.kaggle.com/code/iqmansingh/optiver-4-fold-time-series-split-ensemble), although this borrows the `reduce_mem_usage` and `imbalance_features` snippets as well. The core idea is still to build a voting ensemble on time series splits, but with score tracking so it can reject models that degrade performance.\n",
    "\n",
    "*Note: I eventually learned that scikit-learn has a built-in [`VotingRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html) class, **(shout-out to [@chinzorigtganbat's](https://www.kaggle.com/chinzorigtganbat) [VotingRegressor + Boosters](https://www.kaggle.com/code/chinzorigtganbat/votingregressor-boosters))**, but it's different enough that I couldn't use it here without a rewrite.*\n",
    "\n",
    "*Note2: I also discovered the [many selective ensemble papers](https://scholar.google.com/scholar?q=selective+ensemble+machine+learning&hl=en&as_sdt=0&as_vis=1&oi=scholart) put out in the last decade. At best, this is a naive implementation of those ideas, but I want to acknowldge the authors for their work.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a485a9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_TRAIN = True # true -> train ensemble; false -> load pretrained ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0344a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import joblib\n",
    "# import typing\n",
    "import warnings\n",
    "import itertools\n",
    "warnings.simplefilter('ignore') # ignore FutureWarnings; must precede pandas import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cat\n",
    "import sklearn.metrics as met\n",
    "import sklearn.model_selection as sel\n",
    "import typing_extensions as ext # used over vanilla typing since it backports 3.11+ features\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # ignore bugged CUDA errors; must precede tf import\n",
    "import tensorflow as tf\n",
    "tf.keras.utils.disable_interactive_logging() # ensemble will provide its own condensed version\n",
    "print(('GPU available.' if len(tf.config.list_physical_devices('GPU')) > 0 else 'No GPU detected.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dff2d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit(parallel=True)\n",
    "def compute_triplet_imbalance(values:np.ndarray, combo_indices:list[tuple[int, int, int]]) -> np.ndarray:\n",
    "    num_rows = values.shape[0]\n",
    "    num_combinations = len(combo_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "    for i in nb.prange(num_combinations): # enumerate() works but prange() lets us run in parallel\n",
    "        a, b, c = combo_indices[i]\n",
    "        for j in nb.prange(num_rows):\n",
    "            _a, _b, _c = values[j, a], values[j, b], values[j, c]\n",
    "            max_val = max(_a, _b, _c)\n",
    "            min_val = min(_a, _b, _c)\n",
    "            mid_val = sum([_a, _b, _c])-max_val-min_val\n",
    "            imbalance_features[j, i] = np.nan if mid_val == min_val else (max_val-mid_val)/(mid_val-min_val)\n",
    "    return imbalance_features   \n",
    "\n",
    "def calculate_triplet_imbalance_numba(cols:list[str], data:pd.DataFrame) -> pd.DataFrame:\n",
    "    values = data[cols].values\n",
    "    combo_indices = []\n",
    "    columns = []\n",
    "    for a, b, c in itertools.combinations(cols, 3):\n",
    "        combo_indices.append(tuple([cols.index(col) for col in [a, b, c]]))\n",
    "        columns.append(f'{a}_{b}_{c}_imbalance')\n",
    "    features_array = compute_triplet_imbalance(values, combo_indices)\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "    return features\n",
    "\n",
    "def imbalance_features(data:pd.DataFrame) -> pd.DataFrame:\n",
    "    prices = [*[col for col in data.columns if 'price' in col], 'wap']\n",
    "    sizes = [col for col in data.columns if 'size' in col]\n",
    "    data['volume'] = data.eval('ask_size+bid_size')\n",
    "    data['mid_price'] = data.eval('(ask_price+bid_price)/2')\n",
    "    data['liquidity_imbalance'] = data.eval('(bid_size-ask_size)/volume')\n",
    "    data['matched_imbalance'] = data.eval('(imbalance_size-matched_size)/(imbalance_size+matched_size)')\n",
    "    data['size_imbalance'] = data.eval('bid_size/ask_size')\n",
    "    data['imbalance_momentum'] = data.groupby(level='stock_id').imbalance_size.diff(periods=1) / data.matched_size\n",
    "    data['price_spread'] = data.eval('ask_price-bid_price')\n",
    "    data['spread_intensity'] = data.groupby(level='stock_id').price_spread.diff()\n",
    "    data['price_pressure'] = data.eval('imbalance_size*price_spread')\n",
    "    data['market_urgency'] = data.eval('price_spread*liquidity_imbalance')\n",
    "    data['depth_pressure'] = data.eval('(ask_size-bid_size)*(far_price-near_price)')\n",
    "    for cols in itertools.combinations(prices, 2):\n",
    "        data[f'{cols[0]}_{cols[1]}_imbalance'] = data.eval(f'({cols[0]}-{cols[1]})/({cols[0]}+{cols[1]})')\n",
    "    for cols in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n",
    "        triplet_feature = calculate_triplet_imbalance_numba(cols, data)\n",
    "        data[triplet_feature.columns] = triplet_feature.values\n",
    "    for func in ['mean', 'std', 'skew', 'kurt']:\n",
    "        data[f'all_prices_{func}'] = data[prices].agg(func, axis=1)\n",
    "        data[f'all_sizes_{func}'] = data[sizes].agg(func, axis=1)\n",
    "    for win in [1, 2, 3, 5, 8, 13]:\n",
    "        for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n",
    "            data[f'{col}_shift_{win}'] = data.groupby(level='stock_id')[col].shift(win)\n",
    "            data[f'{col}_pct_{win}'] = data.groupby(level='stock_id')[col].pct_change(win)\n",
    "        for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size', 'market_urgency', 'imbalance_momentum', 'size_imbalance']:\n",
    "            data[f'{col}_diff_{win}'] = data.groupby(level='stock_id')[col].diff(win)\n",
    "    data = data.replace([np.inf, -np.inf], 0)\n",
    "    return data\n",
    "\n",
    "def reduce_mem_usage(data:pd.DataFrame, verbose:bool=False) -> pd.DataFrame: # 3.10+\n",
    "    if verbose: mem_start = data.memory_usage().sum()\n",
    "    for col in data.columns:\n",
    "        match data[col].dtype:\n",
    "            case 'object' | 'bool': continue\n",
    "            case 'int32' | 'int64':\n",
    "                for int_size in [np.int8, np.int16, np.int32]:\n",
    "                    if data[col].min() > np.iinfo(int_size).min and data[col].max() < np.iinfo(int_size).max:\n",
    "                        data[col] = data[col].astype(int_size)\n",
    "            case 'float32' | 'float64':\n",
    "                for float_size in [np.float16, np.float32]:\n",
    "                    if data[col].min() > np.finfo(float_size).min and data[col].max() < np.finfo(float_size).max:\n",
    "                        data[col] = data[col].astype(float_size)\n",
    "            case _: raise Exception(data[col].dtype)\n",
    "    if verbose:\n",
    "        mem_end = data.memory_usage().sum()\n",
    "        print(f'DataFrame memory reduced from {mem_start} to {mem_end}.')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "483b0856",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_DATA_TRAIN = '.data/train.csv'\n",
    "LOCAL_DATA_TEST_X = '.data/test.csv'\n",
    "LOCAL_DATA_TEST_Y = '.data/revealed_targets.csv'\n",
    "\n",
    "KAGGLE_DATA_TRAIN = '/kaggle/input/optiver-trading-at-the-close/train.csv'\n",
    "KAGGLE_DATA_TEST_X = '/kaggle/input/optiver-trading-at-the-close/example_test_files/test.csv'\n",
    "KAGGLE_DATA_TEST_Y = '/kaggle/input/optiver-trading-at-the-close/example_test_files/revealed_targets.csv'\n",
    "\n",
    "DROPS = ['index', 'time_id', 'currently_scored', 'time_id_x', 'time_id_y', 'revealed_date_id', 'revealed_time_id', 'row_id']\n",
    "SORTS = ['date_id', 'stock_id', 'seconds_in_bucket'] # order matters here\n",
    "SKIPS = ['imbalance_buy_sell_flag', 'target']\n",
    "\n",
    "def preprocess(data:pd.DataFrame) -> pd.DataFrame: # separate from load_data() for submission compat\n",
    "    data = data.set_index(SORTS).sort_index()      # pushing these into a multi-index makes life easier down the road\n",
    "    data = imbalance_features(data)                # must precede standardization; requires SKIPS in data\n",
    "    skip = data[[col for col in SKIPS if col in data.columns]]\n",
    "    data = data.drop([col for col in [*DROPS, *SKIPS] if col in data.columns], axis=1)\n",
    "    data = data.groupby(level='stock_id').ffill()  # impute with last observation; groupby() ensures ffill() is per-stock, per-day\n",
    "    data = (data - data.mean()) / data.std(ddof=0) # normalize/standardize (z-score)\n",
    "    data = data.fillna(0)                          # clean columns that didn't ffill or with a stdev of 0 (i.e., only 1 unique value)\n",
    "    data = pd.concat([skip, data], axis=1, join='inner') # re-join with skipped columns\n",
    "    temp = data.index.to_frame().seconds_in_bucket       # encode seconds as sin/cos waves\n",
    "    data['seconds_in_bucket_sin'] = np.sin((temp * 2 * np.pi / 540))\n",
    "    data['seconds_in_bucket_cos'] = np.cos((temp * 2 * np.pi / 540))\n",
    "    return data\n",
    "\n",
    "def load_vars(test:bool=False) -> tuple[pd.DataFrame, pd.Series]: # returns training (or test) data for either local or kaggle setup\n",
    "    def read_data(train, test_x, test_y): # wrap call to read_csv() since test X and y values are stored separately and must be merged\n",
    "        if test: return pd.merge(*[pd.read_csv(path) for path in [test_x, test_y]], on=SORTS).rename(columns={'revealed_target':'target'})\n",
    "        else: return pd.read_csv(train)\n",
    "    try: data = read_data(LOCAL_DATA_TRAIN, LOCAL_DATA_TEST_X, LOCAL_DATA_TEST_Y)\n",
    "    except FileNotFoundError: data = read_data(KAGGLE_DATA_TRAIN, KAGGLE_DATA_TEST_X, KAGGLE_DATA_TEST_Y)\n",
    "    data = data.dropna(subset=['target']) # some rows have null targets\n",
    "    data = reduce_mem_usage(data) # must precede preprocess() or kaggle will run out of memory\n",
    "    data = preprocess(data)\n",
    "    return data.drop('target', axis=1), data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27839d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionError(Exception): pass # specific error type for better training feedback\n",
    "\n",
    "class IModel(ext.Protocol): # partial wrapper for sklearn API\n",
    "    def fit(self, X, y, **kwargs) -> ext.Self: ...\n",
    "    def predict(self, X, **kwargs) -> np.ndarray: ...\n",
    "    def get_params(self, deep=True) -> dict[str, ext.Any]: ...\n",
    "\n",
    "class SelectiveEnsemble: # once len(models) >= limit, reject new models with scores above the mean\n",
    "    def __init__(self, limit:int=None) -> None:\n",
    "        self.limit = limit \n",
    "        self.models = dict[str, IModel]()\n",
    "        self.scores = dict[str, float]()\n",
    "        self.kwargs = dict[str, dict]()\n",
    "        self.test_x, self.test_y = load_vars(test=True)\n",
    "    \n",
    "    @property\n",
    "    def mean_score(self) -> float:\n",
    "        return sum(self.scores[m] for m in self.models) / len(self) if len(self) > 0 else None\n",
    "    \n",
    "    @property\n",
    "    def best_score(self) -> float:\n",
    "        return min(self.scores[m] for m in self.models) if len(self) > 0 else None\n",
    "    \n",
    "    @property\n",
    "    def best_model(self) -> IModel:\n",
    "        return [self.models[m] for m in self.models if self.scores[m] == self.best_score][0]\n",
    "    \n",
    "    def add(self, model:IModel, name:str, kwargs:dict) -> tuple[bool, float]: # raises PredictionError\n",
    "        if name in self.models: name = f'{name}(1)'\n",
    "        pred = model.predict(self.test_x, **kwargs)\n",
    "        if len(np.unique(pred)) == 1: raise PredictionError('Model is guessing a constant value.')\n",
    "        if np.isnan(pred).any(): raise PredictionError('Model is guessing NaN.')\n",
    "        score = met.mean_absolute_error(self.test_y, pred)\n",
    "        if self.limit and len(self) >= self.limit and self.mean_score < score: return False, score\n",
    "        self.models[name] = model\n",
    "        self.scores[name] = score\n",
    "        self.kwargs[name] = kwargs\n",
    "        return True, score\n",
    "\n",
    "    def prune(self, limit:int=None) -> ext.Self: # removes models with scores above the mean; recurses if limit is set\n",
    "        pruned = SelectiveEnsemble(limit=(limit or self.limit))\n",
    "        pruned.models = {m:self.models[m] for m in self.models if self.scores[m] <= self.mean_score}\n",
    "        pruned.scores = {m:self.scores[m] for m in pruned.models}\n",
    "        pruned.kwargs = {m:self.kwargs[m] for m in pruned.models}\n",
    "        if pruned.limit and len(pruned) > pruned.limit > 1: return pruned.prune()\n",
    "        return pruned\n",
    "    \n",
    "    def clone(self, limit:int=None) -> ext.Self:\n",
    "        clone = SelectiveEnsemble(limit=(limit or self.limit))\n",
    "        clone.models = self.models.copy()\n",
    "        clone.scores = self.scores.copy()\n",
    "        clone.kwargs = self.kwargs.copy()\n",
    "        return clone\n",
    "    \n",
    "    def predict(self, X:pd.DataFrame, **kwargs) -> np.ndarray: # wrapper for soft voting; kwargs for compat\n",
    "        y = np.zeros(len(X))\n",
    "        for m in self.models:\n",
    "            pred = self.models[m].predict(X, **self.kwargs[m])\n",
    "            y += pred.reshape(-1) # reshape needed for tensorflow output; doesn't impact other model types\n",
    "        y = y / len(self)\n",
    "        return y\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.models)\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f'<SelectiveEnsemble ({len(self)} model(s); mean: {self.mean_score:.8f}; best: {self.best_score:.8f}; limit: {self.limit})>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dc3d5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FOLDER = '.models/'\n",
    "if not os.path.exists(MODEL_FOLDER): os.makedirs(MODEL_FOLDER)\n",
    "\n",
    "# customize fit() and predict() kwargs for each model's type and params\n",
    "def build_model_kwargs(model:IModel, val_data:tuple[pd.DataFrame, pd.Series]=None) -> tuple[dict, dict, dict]:\n",
    "    fit_kw = dict()\n",
    "    predict_kw = dict()\n",
    "    early_stop_kw = dict()\n",
    "    model_class = type(model).__name__\n",
    "    match model_class:\n",
    "        case 'Sequential':\n",
    "            model.compile(optimizer='adam', loss='mae')\n",
    "            keras_kw = dict(batch_size=256, verbose=0)\n",
    "            fit_kw.update(keras_kw)\n",
    "            predict_kw.update(keras_kw)\n",
    "            early_stop_kw['validation_data'] = val_data\n",
    "        case 'LGBMRegressor':\n",
    "            fit_kw.update(dict(verbose=False)) # verbose=0 throws an error\n",
    "            if 'early_stopping_round' in model.get_params():\n",
    "                early_stop_kw['eval_set'] = [val_data]\n",
    "                early_stop_kw['eval_metric'] = 'l1'\n",
    "        case 'XGBRegressor' | 'CatBoostRegressor':\n",
    "            fit_kw.update(dict(verbose=0))\n",
    "            if 'early_stopping_rounds' in model.get_params():\n",
    "                early_stop_kw['eval_set'] = [val_data]\n",
    "    fit_kw.update(early_stop_kw)\n",
    "    return fit_kw, predict_kw, early_stop_kw\n",
    "\n",
    "# builds an ensemble trained on the data from load_vars(). if an existing ensemble is provided, it will be updated instead.\n",
    "def train_ensemble(models:list[IModel], folds:int=5, limit:int=None, ensemble:SelectiveEnsemble=None, skip_pred_errors:bool=True, ignore_errors:bool=True) -> SelectiveEnsemble:\n",
    "    print(f'Pre-training setup...', end='\\r')\n",
    "    ensemble = ensemble.clone(limit) if ensemble else SelectiveEnsemble(limit=limit)\n",
    "    cv = sel.TimeSeriesSplit(folds)\n",
    "    X, y = load_vars()\n",
    "    for j, model in enumerate(models):\n",
    "        model_class = type(model).__name__\n",
    "        fails = 0 # track consecutive failures\n",
    "        for i, (i_train, i_valid) in enumerate(cv.split(X)):\n",
    "            name = f'{model_class}_{int(time.time())}' # index_class_fold_timestamp\n",
    "            msg = f'Model {j+1}/{len(models)}: Fold {i+1}/{folds}: {name}'\n",
    "            print(f'{msg} - Training...', end='\\r')\n",
    "            try: # fail gracefully instead of giving up on the whole ensemble\n",
    "                X_valid, y_valid = X.iloc[i_valid, :], y.iloc[i_valid]\n",
    "                fit_kw, predict_kw, early_stop_kw = build_model_kwargs(model, (X_valid, y_valid)) \n",
    "                try: model.fit(X.iloc[i_train, :], y.iloc[i_train], **fit_kw) # some kwargs fail on kaggle\n",
    "                except: model.fit(X.iloc[i_train, :], y.iloc[i_train], **early_stop_kw) # fallback to early stop only\n",
    "                del X_valid, y_valid\n",
    "                print(f'{msg} - Submitting to ensemble...', end='\\r')\n",
    "                if model_class == 'Sequential':\n",
    "                    clone = tf.keras.models.clone_model(model)\n",
    "                    clone.set_weights(model.get_weights())\n",
    "                else: clone = None\n",
    "                res, score = ensemble.add((clone or model), name, predict_kw)\n",
    "                print(f'{msg} - {(\"Accepted\" if res else \"Rejected\")} with score: {score:.8f}\\n    {ensemble}') # spacing is intentional\n",
    "                if (res):\n",
    "                    print(f'    Saving to file...', end='\\r')\n",
    "                    save_path = os.path.join(MODEL_FOLDER, f'{name}.joblib')\n",
    "                    joblib.dump(model, save_path)\n",
    "                    print(f'    Saved to {save_path}')\n",
    "                fails = 0\n",
    "            except PredictionError as e: # these tend not to improve, so move on to next model\n",
    "                print(f'{msg} - Stopped: {e}')\n",
    "                if skip_pred_errors: break\n",
    "            except Exception as e: # these are mostly out of memory errors, which can generally be ignored\n",
    "                if not ignore_errors: raise e # ...generally\n",
    "                print(f'{msg} - Error: {type(e).__name__}: {e}')\n",
    "                fails += 1\n",
    "                if fails > 1: break # consecutive failures are usually a model misconfig, skip these as well\n",
    "            finally: gc.collect() # memory is at a premium\n",
    "    return ensemble\n",
    "\n",
    "def load_ensemble(model_dir:str=MODEL_FOLDER) -> SelectiveEnsemble:\n",
    "    ensemble = SelectiveEnsemble()\n",
    "    for file in os.listdir(model_dir):\n",
    "        model = joblib.load(os.path.join(model_dir, file))\n",
    "        name = file.split('.joblib')[0]\n",
    "        kwargs = build_model_kwargs(model, (ensemble.test_x, ensemble.test_y))[1] # only need predict_kw\n",
    "        ensemble.add(model, name, kwargs)\n",
    "    if len(ensemble) == 0: raise FileNotFoundError(f'No models saved in {model_dir}.')\n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ddc476b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1/11: Fold 1/5: XGBRegressor_1702420456 - Accepted with score: 5.55306292\n",
      "    <SelectiveEnsemble (1 model(s); mean: 5.55306292; best: 5.55306292; limit: 21)>\n",
      "    Saved to .models/XGBRegressor_1702420456.joblib\n",
      "Model 1/11: Fold 2/5: XGBRegressor_1702420463 - Accepted with score: 5.52157068\n",
      "    <SelectiveEnsemble (2 model(s); mean: 5.53731680; best: 5.52157068; limit: 21)>\n",
      "    Saved to .models/XGBRegressor_1702420463.joblib\n",
      "Model 1/11: Fold 3/5: XGBRegressor_1702420473 - Accepted with score: 5.51487589\n",
      "    <SelectiveEnsemble (3 model(s); mean: 5.52983650; best: 5.51487589; limit: 21)>\n",
      "    Saved to .models/XGBRegressor_1702420473.joblib\n",
      "Model 1/11: Fold 4/5: XGBRegressor_1702420486 - Accepted with score: 5.49965906\n",
      "    <SelectiveEnsemble (4 model(s); mean: 5.52229214; best: 5.49965906; limit: 21)>\n",
      "    Saved to .models/XGBRegressor_1702420486.joblib\n",
      "Model 1/11: Fold 5/5: XGBRegressor_1702420503 - Accepted with score: 5.45572758\n",
      "    <SelectiveEnsemble (5 model(s); mean: 5.50897923; best: 5.45572758; limit: 21)>\n",
      "    Saved to .models/XGBRegressor_1702420503.joblib\n",
      "Model 2/11: Fold 1/5: LGBMRegressor_1702420515 - Accepted with score: 5.60445984\n",
      "    <SelectiveEnsemble (6 model(s); mean: 5.52489266; best: 5.45572758; limit: 21)>\n",
      "    Saved to .models/LGBMRegressor_1702420515.joblib\n",
      "Model 2/11: Fold 2/5: LGBMRegressor_1702420520 - Accepted with score: 5.56570224\n",
      "    <SelectiveEnsemble (7 model(s); mean: 5.53072260; best: 5.45572758; limit: 21)>\n",
      "    Saved to .models/LGBMRegressor_1702420520.joblib\n",
      "Model 2/11: Fold 3/5: LGBMRegressor_1702420527 - Accepted with score: 5.54015602\n",
      "    <SelectiveEnsemble (8 model(s); mean: 5.53190178; best: 5.45572758; limit: 21)>\n",
      "    Saved to .models/LGBMRegressor_1702420527.joblib\n",
      "Model 2/11: Fold 4/5: LGBMRegressor_1702420538 - Accepted with score: 5.52625253\n",
      "    <SelectiveEnsemble (9 model(s); mean: 5.53127408; best: 5.45572758; limit: 21)>\n",
      "    Saved to .models/LGBMRegressor_1702420538.joblib\n",
      "Model 2/11: Fold 5/5: LGBMRegressor_1702420552 - Accepted with score: 5.47733110\n",
      "    <SelectiveEnsemble (10 model(s); mean: 5.52587979; best: 5.45572758; limit: 21)>\n",
      "    Saved to .models/LGBMRegressor_1702420552.joblib\n",
      "Model 3/11: Fold 1/5: CatBoostRegressor_1702420562 - Accepted with score: 5.67475530\n",
      "    <SelectiveEnsemble (11 model(s); mean: 5.53941392; best: 5.45572758; limit: 21)>\n",
      "    Saved to .models/CatBoostRegressor_1702420562.joblib\n",
      "Model 3/11: Fold 2/5: CatBoostRegressor_1702420565 - Accepted with score: 5.48738879\n",
      "    <SelectiveEnsemble (12 model(s); mean: 5.53507850; best: 5.45572758; limit: 21)>\n",
      "    Saved to .models/CatBoostRegressor_1702420565.joblib\n",
      "Model 3/11: Fold 3/5: CatBoostRegressor_1702420568 - Accepted with score: 5.47980107\n",
      "    <SelectiveEnsemble (13 model(s); mean: 5.53082639; best: 5.45572758; limit: 21)>\n",
      "    Saved to .models/CatBoostRegressor_1702420568.joblib\n",
      "Model 3/11: Fold 4/5: CatBoostRegressor_1702420572 - Accepted with score: 5.46246080\n",
      "    <SelectiveEnsemble (14 model(s); mean: 5.52594313; best: 5.45572758; limit: 21)>\n",
      "    Saved to .models/CatBoostRegressor_1702420572.joblib\n",
      "Model 3/11: Fold 5/5: CatBoostRegressor_1702420577 - Accepted with score: 5.45899006\n",
      "    <SelectiveEnsemble (15 model(s); mean: 5.52147959; best: 5.45572758; limit: 21)>\n",
      "    Saved to .models/CatBoostRegressor_1702420577.joblib\n",
      "Model 4/11: Fold 1/5: Sequential_1702420586 - Training...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1702420588.408567  153612 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 4/11: Fold 1/5: Sequential_1702420586 - Accepted with score: 5.42390680\n",
      "    <SelectiveEnsemble (16 model(s); mean: 5.51538129; best: 5.42390680; limit: 21)>\n",
      "    Saved to .models/Sequential_1702420586.joblib\n",
      "Model 4/11: Fold 2/5: Sequential_1702420593 - Accepted with score: 5.42758036\n",
      "    <SelectiveEnsemble (17 model(s); mean: 5.51021653; best: 5.42390680; limit: 21)>\n",
      "    Saved to .models/Sequential_1702420593.joblib\n",
      "Model 4/11: Fold 3/5: Sequential_1702420603 - Accepted with score: 5.42931509\n",
      "    <SelectiveEnsemble (18 model(s); mean: 5.50572201; best: 5.42390680; limit: 21)>\n",
      "    Saved to .models/Sequential_1702420603.joblib\n",
      "Model 4/11: Fold 4/5: Sequential_1702420616 - Accepted with score: 5.42478704\n",
      "    <SelectiveEnsemble (19 model(s); mean: 5.50146227; best: 5.42390680; limit: 21)>\n",
      "    Saved to .models/Sequential_1702420616.joblib\n",
      "Model 4/11: Fold 5/5: Sequential_1702420632 - Accepted with score: 5.43136692\n",
      "    <SelectiveEnsemble (20 model(s); mean: 5.49795750; best: 5.42390680; limit: 21)>\n",
      "    Saved to .models/Sequential_1702420632.joblib\n",
      "Model 5/11: Fold 1/5: Sequential_1702420651 - Accepted with score: 5.43672991\n",
      "    <SelectiveEnsemble (21 model(s); mean: 5.49504190; best: 5.42390680; limit: 21)>\n",
      "    Saved to .models/Sequential_1702420651.joblib\n",
      "Model 5/11: Fold 2/5: Sequential_1702420662 - Accepted with score: 5.45935965\n",
      "    <SelectiveEnsemble (22 model(s); mean: 5.49341998; best: 5.42390680; limit: 21)>\n",
      "    Saved to .models/Sequential_1702420662.joblib\n",
      "Model 5/11: Fold 3/5: Sequential_1702420677 - Accepted with score: 5.45433283\n",
      "    <SelectiveEnsemble (23 model(s); mean: 5.49172054; best: 5.42390680; limit: 21)>\n",
      "    Saved to .models/Sequential_1702420677.joblib\n",
      "Model 5/11: Fold 4/5: Sequential_1702420698 - Accepted with score: 5.45876074\n",
      "    <SelectiveEnsemble (24 model(s); mean: 5.49034722; best: 5.42390680; limit: 21)>\n",
      "    Saved to .models/Sequential_1702420698.joblib\n",
      "Model 5/11: Fold 5/5: Sequential_1702420723 - Accepted with score: 5.45417500\n",
      "    <SelectiveEnsemble (25 model(s); mean: 5.48890033; best: 5.42390680; limit: 21)>\n",
      "    Saved to .models/Sequential_1702420723.joblib\n",
      "Model 6/11: Fold 1/5: Sequential_1702420756 - Accepted with score: 5.44082832\n",
      "    <SelectiveEnsemble (26 model(s); mean: 5.48705141; best: 5.42390680; limit: 21)>\n",
      "    Saved to .models/Sequential_1702420756.joblib\n",
      "Model 6/11: Fold 2/5: Sequential_1702420765 - Accepted with score: 5.47459841\n",
      "    <SelectiveEnsemble (27 model(s); mean: 5.48659018; best: 5.42390680; limit: 21)>\n",
      "    Saved to .models/Sequential_1702420765.joblib\n",
      "Model 6/11: Fold 3/5: Sequential_1702420779 - Accepted with score: 5.44531155\n",
      "    <SelectiveEnsemble (28 model(s); mean: 5.48511595; best: 5.42390680; limit: 21)>\n",
      "    Saved to .models/Sequential_1702420779.joblib\n",
      "Model 6/11: Fold 4/5: Sequential_1702420799 - Accepted with score: 5.45933437\n",
      "    <SelectiveEnsemble (29 model(s); mean: 5.48422693; best: 5.42390680; limit: 21)>\n",
      "    Saved to .models/Sequential_1702420799.joblib\n",
      "Model 6/11: Fold 5/5: Sequential_1702420825 - Accepted with score: 5.45695782\n",
      "    <SelectiveEnsemble (30 model(s); mean: 5.48331796; best: 5.42390680; limit: 21)>\n",
      "    Saved to .models/Sequential_1702420825.joblib\n",
      "Model 7/11: Fold 1/5: Sequential_1702420856 - Accepted with score: 5.46895838\n",
      "    <SelectiveEnsemble (31 model(s); mean: 5.48285474; best: 5.42390680; limit: 21)>\n",
      "    Saved to .models/Sequential_1702420856.joblib\n",
      "Model 7/11: Fold 2/5: Sequential_1702420865 - Accepted with score: 5.46671820\n",
      "    <SelectiveEnsemble (32 model(s); mean: 5.48235048; best: 5.42390680; limit: 21)>\n",
      "    Saved to .models/Sequential_1702420865.joblib\n",
      "Model 7/11: Fold 3/5: Sequential_1702420879 - Accepted with score: 5.45084429\n",
      "    <SelectiveEnsemble (33 model(s); mean: 5.48139574; best: 5.42390680; limit: 21)>\n",
      "    Saved to .models/Sequential_1702420879.joblib\n",
      "Model 7/11: Fold 4/5: Sequential_1702420898 - Accepted with score: 5.45463085\n",
      "    <SelectiveEnsemble (34 model(s); mean: 5.48060854; best: 5.42390680; limit: 21)>\n",
      "    Saved to .models/Sequential_1702420898.joblib\n",
      "Model 7/11: Fold 5/5: Sequential_1702420926 - Accepted with score: 5.44443941\n",
      "    <SelectiveEnsemble (35 model(s); mean: 5.47957514; best: 5.42390680; limit: 21)>\n",
      "    Saved to .models/Sequential_1702420926.joblib\n",
      "Model 8/11: Fold 1/5: Sequential_1702420958 - Accepted with score: 5.45032072\n",
      "    <SelectiveEnsemble (36 model(s); mean: 5.47876251; best: 5.42390680; limit: 21)>\n",
      "    Saved to .models/Sequential_1702420958.joblib\n",
      "Model 8/11: Fold 2/5: Sequential_1702420967 - Accepted with score: 5.47662735\n",
      "    <SelectiveEnsemble (37 model(s); mean: 5.47870481; best: 5.42390680; limit: 21)>\n",
      "    Saved to .models/Sequential_1702420967.joblib\n",
      "Model 8/11: Fold 3/5: Sequential_1702420981 - Accepted with score: 5.44584370\n",
      "    <SelectiveEnsemble (38 model(s); mean: 5.47784004; best: 5.42390680; limit: 21)>\n",
      "    Saved to .models/Sequential_1702420981.joblib\n",
      "Model 8/11: Fold 4/5: Sequential_1702421002 - Accepted with score: 5.44752932\n",
      "    <SelectiveEnsemble (39 model(s); mean: 5.47706284; best: 5.42390680; limit: 21)>\n",
      "    Saved to .models/Sequential_1702421002.joblib\n",
      "Model 8/11: Fold 5/5: Sequential_1702421027 - Accepted with score: 5.45116568\n",
      "    <SelectiveEnsemble (40 model(s); mean: 5.47641541; best: 5.42390680; limit: 21)>\n",
      "    Saved to .models/Sequential_1702421027.joblib\n",
      "Model 9/11: Fold 1/5: Sequential_1702421057 - Accepted with score: 5.40862560\n",
      "    <SelectiveEnsemble (41 model(s); mean: 5.47476200; best: 5.40862560; limit: 21)>\n",
      "    Saved to .models/Sequential_1702421057.joblib\n",
      "Model 9/11: Fold 2/5: Sequential_1702421067 - Accepted with score: 5.43243408\n",
      "    <SelectiveEnsemble (42 model(s); mean: 5.47375420; best: 5.40862560; limit: 21)>\n",
      "    Saved to .models/Sequential_1702421067.joblib\n",
      "Model 9/11: Fold 3/5: Sequential_1702421083 - Accepted with score: 5.40864277\n",
      "    <SelectiveEnsemble (43 model(s); mean: 5.47223998; best: 5.40862560; limit: 21)>\n",
      "    Saved to .models/Sequential_1702421083.joblib\n",
      "Model 9/11: Fold 4/5: Sequential_1702421105 - Accepted with score: 5.40846634\n",
      "    <SelectiveEnsemble (44 model(s); mean: 5.47079058; best: 5.40846634; limit: 21)>\n",
      "    Saved to .models/Sequential_1702421105.joblib\n",
      "Model 9/11: Fold 5/5: Sequential_1702421135 - Accepted with score: 5.41451931\n",
      "    <SelectiveEnsemble (45 model(s); mean: 5.46954010; best: 5.40846634; limit: 21)>\n",
      "    Saved to .models/Sequential_1702421135.joblib\n",
      "Model 10/11: Fold 1/5: Sequential_1702421170 - Accepted with score: 5.43394518\n",
      "    <SelectiveEnsemble (46 model(s); mean: 5.46876630; best: 5.40846634; limit: 21)>\n",
      "    Saved to .models/Sequential_1702421170.joblib\n",
      "Model 10/11: Fold 2/5: Sequential_1702421182 - Accepted with score: 5.43037891\n",
      "    <SelectiveEnsemble (47 model(s); mean: 5.46794955; best: 5.40846634; limit: 21)>\n",
      "    Saved to .models/Sequential_1702421182.joblib\n",
      "Model 10/11: Fold 3/5: Sequential_1702421203 - Accepted with score: 5.41736937\n",
      "    <SelectiveEnsemble (48 model(s); mean: 5.46689579; best: 5.40846634; limit: 21)>\n",
      "    Saved to .models/Sequential_1702421203.joblib\n",
      "Model 10/11: Fold 4/5: Sequential_1702421231 - Accepted with score: 5.42057467\n",
      "    <SelectiveEnsemble (49 model(s); mean: 5.46595047; best: 5.40846634; limit: 21)>\n",
      "    Saved to .models/Sequential_1702421231.joblib\n",
      "Model 10/11: Fold 5/5: Sequential_1702421264 - Accepted with score: 5.41114664\n",
      "    <SelectiveEnsemble (50 model(s); mean: 5.46485439; best: 5.40846634; limit: 21)>\n",
      "    Saved to .models/Sequential_1702421264.joblib\n",
      "Model 11/11: Fold 1/5: Sequential_1702421308 - Accepted with score: 5.42553473\n",
      "    <SelectiveEnsemble (51 model(s); mean: 5.46408341; best: 5.40846634; limit: 21)>\n",
      "    Saved to .models/Sequential_1702421308.joblib\n",
      "Model 11/11: Fold 2/5: Sequential_1702421320 - Accepted with score: 5.45070934\n",
      "    <SelectiveEnsemble (52 model(s); mean: 5.46382622; best: 5.40846634; limit: 21)>\n",
      "    Saved to .models/Sequential_1702421320.joblib\n",
      "Model 11/11: Fold 3/5: Sequential_1702421341 - Accepted with score: 5.42549753\n",
      "    <SelectiveEnsemble (53 model(s); mean: 5.46310304; best: 5.40846634; limit: 21)>\n",
      "    Saved to .models/Sequential_1702421341.joblib\n",
      "Model 11/11: Fold 4/5: Sequential_1702421370 - Accepted with score: 5.44362354\n",
      "    <SelectiveEnsemble (54 model(s); mean: 5.46274231; best: 5.40846634; limit: 21)>\n",
      "    Saved to .models/Sequential_1702421370.joblib\n",
      "Model 11/11: Fold 5/5: Sequential_1702421405 - Accepted with score: 5.42503929\n",
      "    <SelectiveEnsemble (55 model(s); mean: 5.46205680; best: 5.40846634; limit: 21)>\n",
      "    Saved to .models/Sequential_1702421405.joblib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SelectiveEnsemble (55 model(s); mean: 5.46205680; best: 5.40846634; limit: 21)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_FEATURES = len(load_vars(test=True)[0].columns)\n",
    "ACTIVATION_1 = 'tanh' # inputs are standardized so keep negative range\n",
    "ACTIVATION_2 = 'relu' # performed better than tanh, sigmoid\n",
    "DROPOUT = 0.5         # performed better than 0.3, 0.4\n",
    "RANDOM_STATE = 25     # funnier than 24\n",
    "\n",
    "layers = tf.keras.layers\n",
    "Sequential = tf.keras.Sequential\n",
    "regularizer = tf.keras.regularizers.l1(0.001)\n",
    "tf.keras.utils.set_random_seed(RANDOM_STATE)\n",
    "shared_params = dict(random_state=RANDOM_STATE, learning_rate=0.2, max_depth=3, subsample=0.8) # cat/lgb/xgb overlap\n",
    "gb_params = dict(**shared_params, n_jobs=16, colsample_bytree=0.85, reg_alpha=500) # lgb/xgb overlap\n",
    "\n",
    "models = [ # order matters if limit is set; frontloading stronger models will cause more rejections; the reverse will oversaturate\n",
    "    xgb.XGBRegressor(**gb_params, early_stopping_rounds=5, eval_metric='mae', tree_method='hist', gamma=0.2),\n",
    "    lgb.LGBMRegressor(**gb_params, early_stopping_round=5, metric='l1', num_leaves=8, min_child_samples=2000, min_split_gain=0.001, verbosity=-1),\n",
    "    cat.CatBoostRegressor(**shared_params, early_stopping_rounds=5, eval_metric='MAE', num_leaves=8, min_child_samples=2000, verbose=0),\n",
    "    Sequential([layers.Dense(1, activation=ACTIVATION_1, input_shape=[N_FEATURES])]), # 145 -> 1\n",
    "    Sequential([ # 145 -> 145 -> 1\n",
    "        layers.Dense(N_FEATURES, kernel_regularizer=regularizer, activation=ACTIVATION_1, input_shape=[N_FEATURES]),\n",
    "        layers.Dropout(DROPOUT),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(N_FEATURES, kernel_regularizer=regularizer, activation=ACTIVATION_2),\n",
    "        layers.Dropout(DROPOUT),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(1)\n",
    "    ]),\n",
    "    Sequential([ # 145 -> 72 -> 1\n",
    "        layers.Dense(N_FEATURES, kernel_regularizer=regularizer, activation=ACTIVATION_1, input_shape=[N_FEATURES]),\n",
    "        layers.Dropout(DROPOUT),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(N_FEATURES//2, kernel_regularizer=regularizer, activation=ACTIVATION_2),\n",
    "        layers.Dropout(DROPOUT),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(1)\n",
    "    ]), \n",
    "    Sequential([ # 145 -> 36 -> 1\n",
    "        layers.Dense(N_FEATURES, kernel_regularizer=regularizer, activation=ACTIVATION_1, input_shape=[N_FEATURES]),\n",
    "        layers.Dropout(DROPOUT),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(N_FEATURES//4, kernel_regularizer=regularizer, activation=ACTIVATION_2),\n",
    "        layers.Dropout(DROPOUT),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(1)\n",
    "    ]),\n",
    "    Sequential([ # 145 -> 18 -> 1\n",
    "        layers.Dense(N_FEATURES, kernel_regularizer=regularizer, activation=ACTIVATION_1, input_shape=[N_FEATURES]),\n",
    "        layers.Dropout(DROPOUT),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(N_FEATURES//8, kernel_regularizer=regularizer, activation=ACTIVATION_2),\n",
    "        layers.Dropout(DROPOUT),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(1)\n",
    "    ]), \n",
    "    Sequential([ # 145 -> 29 -> 5 -> 1\n",
    "        layers.Dense(N_FEATURES, kernel_regularizer=regularizer, activation=ACTIVATION_1, input_shape=[N_FEATURES]),\n",
    "        layers.Dropout(DROPOUT),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(29, kernel_regularizer=regularizer, activation=ACTIVATION_2),\n",
    "        layers.Dropout(DROPOUT),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(5, kernel_regularizer=regularizer, activation=ACTIVATION_2),\n",
    "        layers.Dropout(DROPOUT),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(1)\n",
    "    ]),\n",
    "    Sequential([ # 145 -> 89 -> 13 -> 5 -> 1 \n",
    "        layers.Dense(N_FEATURES, kernel_regularizer=regularizer, activation=ACTIVATION_1, input_shape=[N_FEATURES]),\n",
    "        layers.Dropout(DROPOUT),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(89, kernel_regularizer=regularizer, activation=ACTIVATION_2),\n",
    "        layers.Dropout(DROPOUT),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(13, kernel_regularizer=regularizer, activation=ACTIVATION_2),\n",
    "        layers.Dropout(DROPOUT),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(5, kernel_regularizer=regularizer, activation=ACTIVATION_2),\n",
    "        layers.Dropout(DROPOUT),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(1)\n",
    "    ]),\n",
    "    Sequential([ # 145 -> 72 -> 36 -> 18 -> 1\n",
    "        layers.Dense(N_FEATURES, kernel_regularizer=regularizer, activation=ACTIVATION_1, input_shape=[N_FEATURES]),\n",
    "        layers.Dropout(DROPOUT),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(N_FEATURES//2, kernel_regularizer=regularizer, activation=ACTIVATION_2),\n",
    "        layers.Dropout(DROPOUT),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(N_FEATURES//4, kernel_regularizer=regularizer, activation=ACTIVATION_2),\n",
    "        layers.Dropout(DROPOUT),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(N_FEATURES//8, kernel_regularizer=regularizer, activation=ACTIVATION_2),\n",
    "        layers.Dropout(DROPOUT),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(1)\n",
    "    ]),\n",
    "]\n",
    "\n",
    "ensemble = train_ensemble(models, limit=max(8, min(len(models), 21))) if IS_TRAIN else load_ensemble() # targeting 8-21 models in ensemble\n",
    "ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f176989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble: 5.462057, 55 models (XGBRegressor_1702420456, XGBRegressor_1702420463, XGBRegressor_1702420473, XGBRegressor_1702420486, XGBRegressor_1702420503, LGBMRegressor_1702420515, LGBMRegressor_1702420520, LGBMRegressor_1702420527, LGBMRegressor_1702420538, LGBMRegressor_1702420552, CatBoostRegressor_1702420562, CatBoostRegressor_1702420565, CatBoostRegressor_1702420568, CatBoostRegressor_1702420572, CatBoostRegressor_1702420577, Sequential_1702420586, Sequential_1702420593, Sequential_1702420603, Sequential_1702420616, Sequential_1702420632, Sequential_1702420651, Sequential_1702420662, Sequential_1702420677, Sequential_1702420698, Sequential_1702420723, Sequential_1702420756, Sequential_1702420765, Sequential_1702420779, Sequential_1702420799, Sequential_1702420825, Sequential_1702420856, Sequential_1702420865, Sequential_1702420879, Sequential_1702420898, Sequential_1702420926, Sequential_1702420958, Sequential_1702420967, Sequential_1702420981, Sequential_1702421002, Sequential_1702421027, Sequential_1702421057, Sequential_1702421067, Sequential_1702421083, Sequential_1702421105, Sequential_1702421135, Sequential_1702421170, Sequential_1702421182, Sequential_1702421203, Sequential_1702421231, Sequential_1702421264, Sequential_1702421308, Sequential_1702421320, Sequential_1702421341, Sequential_1702421370, Sequential_1702421405)\n",
      "Pruned  : 5.422940, 19 models (Sequential_1702420586, Sequential_1702420593, Sequential_1702420603, Sequential_1702420616, Sequential_1702420632, Sequential_1702420651, Sequential_1702421057, Sequential_1702421067, Sequential_1702421083, Sequential_1702421105, Sequential_1702421135, Sequential_1702421170, Sequential_1702421182, Sequential_1702421203, Sequential_1702421231, Sequential_1702421264, Sequential_1702421308, Sequential_1702421341, Sequential_1702421405)\n",
      "Top N   : 5.412764, 7 models (Sequential_1702421057, Sequential_1702421083, Sequential_1702421105, Sequential_1702421135, Sequential_1702421203, Sequential_1702421231, Sequential_1702421264)\n"
     ]
    }
   ],
   "source": [
    "pruned = ensemble.prune()\n",
    "top = ensemble.prune(13)\n",
    "print(f'Ensemble: {ensemble.mean_score:.6f}, {len(ensemble)} models ({\", \".join([m for m in ensemble.models])})')\n",
    "print(f'Pruned  : {pruned.mean_score:.6f}, {len(pruned)} models ({\", \".join([m for m in pruned.models])})')\n",
    "print(f'Top N   : {top.mean_score:.6f}, {len(top)} models ({\", \".join([m for m in top.models])})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a38bd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception # stop for manual eval\n",
    "model = top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2badf9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>478_0_0</td>\n",
       "      <td>-0.396068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>478_0_1</td>\n",
       "      <td>0.389813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>478_0_2</td>\n",
       "      <td>0.509971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>478_0_3</td>\n",
       "      <td>-0.343241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>478_0_4</td>\n",
       "      <td>-0.441371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32995</th>\n",
       "      <td>480_540_195</td>\n",
       "      <td>0.226531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32996</th>\n",
       "      <td>480_540_196</td>\n",
       "      <td>-0.650717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32997</th>\n",
       "      <td>480_540_197</td>\n",
       "      <td>0.363197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32998</th>\n",
       "      <td>480_540_198</td>\n",
       "      <td>0.362143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32999</th>\n",
       "      <td>480_540_199</td>\n",
       "      <td>-0.333332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            row_id    target\n",
       "0          478_0_0 -0.396068\n",
       "1          478_0_1  0.389813\n",
       "2          478_0_2  0.509971\n",
       "3          478_0_3 -0.343241\n",
       "4          478_0_4 -0.441371\n",
       "...            ...       ...\n",
       "32995  480_540_195  0.226531\n",
       "32996  480_540_196 -0.650717\n",
       "32997  480_540_197  0.363197\n",
       "32998  480_540_198  0.362143\n",
       "32999  480_540_199 -0.333332\n",
       "\n",
       "[33000 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import optiver2023\n",
    "env = optiver2023.make_env()\n",
    "iter_test = env.iter_test()\n",
    "\n",
    "for (test, _, _) in iter_test:\n",
    "    X_test = preprocess(test)\n",
    "    y_pred = model.predict(X_test, verbose=0)\n",
    "    submission = test[['row_id']]\n",
    "    submission['target'] = y_pred\n",
    "    env.predict(submission)\n",
    "\n",
    "try:\n",
    "    res = pd.read_csv('/kaggle/working/submission.csv') # sanity check\n",
    "except FileNotFoundError:\n",
    "    res = pd.read_csv('.data/submission.csv')\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
