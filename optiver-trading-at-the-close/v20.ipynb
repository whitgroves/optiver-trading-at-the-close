{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Largely built on [@iqmansingh's](https://www.kaggle.com/iqmansingh) notebook, [4-Fold Time-Series Split Ensemble](https://www.kaggle.com/code/iqmansingh/optiver-4-fold-time-series-split-ensemble), although this borrows the `reduce_mem_usage` and `imbalance_features` snippets as well. The core idea is still to build a voting ensemble on time series splits, but with score tracking so it can reject models that degrade performance.\n",
    "\n",
    "*Note: I eventually learned that scikit-learn has a built-in [`VotingRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html) class, **(shout-out to [@chinzorigtganbat's](https://www.kaggle.com/chinzorigtganbat) [VotingRegressor + Boosters](https://www.kaggle.com/code/chinzorigtganbat/votingregressor-boosters))**, but it's different enough that I couldn't use it here without a rewrite.*\n",
    "\n",
    "*Note2: I also discovered the [many selective ensemble papers](https://scholar.google.com/scholar?q=selective+ensemble+machine+learning&hl=en&as_sdt=0&as_vis=1&oi=scholart) put out in the last decade. At best, this is a naive implementation of those ideas, but I want to acknowldge the authors for their work.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a485a9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_TRAIN = True # true -> train ensemble; false -> load pretrained ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import joblib\n",
    "# import typing\n",
    "import warnings\n",
    "import itertools\n",
    "warnings.simplefilter('ignore') # ignore FutureWarnings; must precede pandas import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cat\n",
    "# import sklearn.svm as svm\n",
    "import sklearn.metrics as met\n",
    "import sklearn.model_selection as sel\n",
    "import typing_extensions as ext # used over vanilla typing since it backports 3.11+ features\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # ignore bugged CUDA errors; must precede tf import\n",
    "import tensorflow as tf\n",
    "tf.keras.utils.disable_interactive_logging() # ensemble will provide its own condensed version\n",
    "print(('GPU available.' if len(tf.config.list_physical_devices('GPU')) > 0 else 'No GPU detected.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit(parallel=True)\n",
    "def compute_triplet_imbalance(values:np.ndarray, combo_indices:list[tuple[int, int, int]]) -> np.ndarray:\n",
    "    num_rows = values.shape[0]\n",
    "    num_combinations = len(combo_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "    for i in nb.prange(num_combinations): # enumerate() works but prange() lets us run in parallel\n",
    "        a, b, c = combo_indices[i]\n",
    "        for j in nb.prange(num_rows):\n",
    "            _a, _b, _c = values[j, a], values[j, b], values[j, c]\n",
    "            max_val = max(_a, _b, _c)\n",
    "            min_val = min(_a, _b, _c)\n",
    "            mid_val = sum([_a, _b, _c])-max_val-min_val\n",
    "            imbalance_features[j, i] = np.nan if mid_val == min_val else (max_val-mid_val)/(mid_val-min_val)\n",
    "    return imbalance_features   \n",
    "\n",
    "def calculate_triplet_imbalance_numba(cols:list[str], data:pd.DataFrame) -> pd.DataFrame:\n",
    "    values = data[cols].values\n",
    "    combo_indices = []\n",
    "    columns = []\n",
    "    for a, b, c in itertools.combinations(cols, 3):\n",
    "        combo_indices.append(tuple([cols.index(col) for col in [a, b, c]]))\n",
    "        columns.append(f'{a}_{b}_{c}_imbalance')\n",
    "    features_array = compute_triplet_imbalance(values, combo_indices)\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "    return features\n",
    "\n",
    "def imbalance_features(data:pd.DataFrame) -> pd.DataFrame:\n",
    "    prices = [*[col for col in data.columns if 'price' in col], 'wap']\n",
    "    sizes = [col for col in data.columns if 'size' in col]\n",
    "    data['volume'] = data.eval('ask_size+bid_size')\n",
    "    data['mid_price'] = data.eval('(ask_price+bid_price)/2')\n",
    "    data['liquidity_imbalance'] = data.eval('(bid_size-ask_size)/volume')\n",
    "    data['matched_imbalance'] = data.eval('(imbalance_size-matched_size)/(imbalance_size+matched_size)')\n",
    "    data['size_imbalance'] = data.eval('bid_size/ask_size')\n",
    "    data['imbalance_momentum'] = data.groupby(level='stock_id').imbalance_size.diff(periods=1) / data.matched_size\n",
    "    data['price_spread'] = data.eval('ask_price-bid_price')\n",
    "    data['spread_intensity'] = data.groupby(level='stock_id').price_spread.diff()\n",
    "    data['price_pressure'] = data.eval('imbalance_size*price_spread')\n",
    "    data['market_urgency'] = data.eval('price_spread*liquidity_imbalance')\n",
    "    data['depth_pressure'] = data.eval('(ask_size-bid_size)*(far_price-near_price)')\n",
    "    for cols in itertools.combinations(prices, 2):\n",
    "        data[f'{cols[0]}_{cols[1]}_imbalance'] = data.eval(f'({cols[0]}-{cols[1]})/({cols[0]}+{cols[1]})')\n",
    "    for cols in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n",
    "        triplet_feature = calculate_triplet_imbalance_numba(cols, data)\n",
    "        data[triplet_feature.columns] = triplet_feature.values\n",
    "    for func in ['mean', 'std', 'skew', 'kurt']:\n",
    "        data[f'all_prices_{func}'] = data[prices].agg(func, axis=1)\n",
    "        data[f'all_sizes_{func}'] = data[sizes].agg(func, axis=1)\n",
    "    for win in [1, 2, 3, 5, 8, 13]:\n",
    "        for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n",
    "            data[f'{col}_shift_{win}'] = data.groupby(level='stock_id')[col].shift(win)\n",
    "            data[f'{col}_pct_{win}'] = data.groupby(level='stock_id')[col].pct_change(win)\n",
    "        for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size', 'market_urgency', 'imbalance_momentum', 'size_imbalance']:\n",
    "            data[f'{col}_diff_{win}'] = data.groupby(level='stock_id')[col].diff(win)\n",
    "    data = data.replace([np.inf, -np.inf], 0)\n",
    "    return data\n",
    "\n",
    "def reduce_mem_usage(data:pd.DataFrame, verbose:bool=False) -> pd.DataFrame: # 3.10+\n",
    "    if verbose: mem_start = data.memory_usage().sum()\n",
    "    for col in data.columns:\n",
    "        match data[col].dtype:\n",
    "            case 'object' | 'bool': continue\n",
    "            case 'int32' | 'int64':\n",
    "                for int_size in [np.int8, np.int16, np.int32]:\n",
    "                    if data[col].min() > np.iinfo(int_size).min and data[col].max() < np.iinfo(int_size).max:\n",
    "                        data[col] = data[col].astype(int_size)\n",
    "            case 'float32' | 'float64':\n",
    "                for float_size in [np.float16, np.float32]:\n",
    "                    if data[col].min() > np.finfo(float_size).min and data[col].max() < np.finfo(float_size).max:\n",
    "                        data[col] = data[col].astype(float_size)\n",
    "            case _: raise Exception(data[col].dtype)\n",
    "    if verbose:\n",
    "        mem_end = data.memory_usage().sum()\n",
    "        print(f'DataFrame memory reduced from {mem_start} to {mem_end}.')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "483b0856",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_DATA_TRAIN = '.data/train.csv'\n",
    "LOCAL_DATA_TEST_X = '.data/test.csv'\n",
    "LOCAL_DATA_TEST_Y = '.data/revealed_targets.csv'\n",
    "\n",
    "KAGGLE_DATA_TRAIN = '/kaggle/input/optiver-trading-at-the-close/train.csv'\n",
    "KAGGLE_DATA_TEST_X = '/kaggle/input/optiver-trading-at-the-close/example_test_files/test.csv'\n",
    "KAGGLE_DATA_TEST_Y = '/kaggle/input/optiver-trading-at-the-close/example_test_files/revealed_targets.csv'\n",
    "\n",
    "DROPS = ['index', 'time_id', 'currently_scored', 'time_id_x', 'time_id_y', 'revealed_date_id', 'revealed_time_id', 'row_id']\n",
    "SORTS = ['date_id', 'stock_id', 'seconds_in_bucket'] # order matters here\n",
    "SKIPS = ['imbalance_buy_sell_flag', 'target']\n",
    "\n",
    "def preprocess(data:pd.DataFrame) -> pd.DataFrame: # separate from load_data() for submission compat\n",
    "    data = data.set_index(SORTS).sort_index()      # pushing these into a multi-index makes life easier down the road\n",
    "    data = imbalance_features(data)                # must precede standardization; requires SKIPS in data\n",
    "    skip = data[[col for col in SKIPS if col in data.columns]]\n",
    "    data = data.drop([col for col in [*DROPS, *SKIPS] if col in data.columns], axis=1)\n",
    "    data = data.groupby(level='stock_id').ffill()  # impute with last observation; groupby() ensures ffill() is per-stock, per-day\n",
    "    data = (data - data.mean()) / data.std(ddof=0) # normalize/standardize (z-score)\n",
    "    data = data.fillna(0)                          # clean columns that didn't ffill or with a stdev of 0 (i.e., only 1 unique value)\n",
    "    data = pd.concat([skip, data], axis=1, join='inner') # re-join with skipped columns\n",
    "    temp = data.index.to_frame().seconds_in_bucket       # encode seconds as sin/cos waves\n",
    "    data['seconds_in_bucket_sin'] = np.sin((temp * 2 * np.pi / 540))\n",
    "    data['seconds_in_bucket_cos'] = np.cos((temp * 2 * np.pi / 540))\n",
    "    return data\n",
    "\n",
    "def load_vars(test:bool=False) -> tuple[pd.DataFrame, pd.Series]: # returns training (or test) data for either local or kaggle setup\n",
    "    def read_data(train, test_x, test_y): # wrap call to read_csv() since test X and y values are stored separately and must be merged\n",
    "        if test: return pd.merge(*[pd.read_csv(path) for path in [test_x, test_y]], on=SORTS).rename(columns={'revealed_target':'target'})\n",
    "        else: return pd.read_csv(train)\n",
    "    try: data = read_data(LOCAL_DATA_TRAIN, LOCAL_DATA_TEST_X, LOCAL_DATA_TEST_Y)\n",
    "    except FileNotFoundError: data = read_data(KAGGLE_DATA_TRAIN, KAGGLE_DATA_TEST_X, KAGGLE_DATA_TEST_Y)\n",
    "    data = data.dropna(subset=['target']) # some rows have null targets\n",
    "    data = reduce_mem_usage(data) # must precede preprocess() or kaggle will run out of memory\n",
    "    data = preprocess(data)\n",
    "    return data.drop('target', axis=1), data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27839d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionError(Exception): pass # specific error type so we can fail gracefully later on during training\n",
    "\n",
    "class IModel(ext.Protocol): # partial wrapper for sklearn API\n",
    "    def fit(self, X, y, **kwargs) -> ext.Self: ...\n",
    "    def predict(self, X, **kwargs) -> np.ndarray: ...\n",
    "    def get_params(self, deep=True) -> dict[str, ext.Any]: ...\n",
    "\n",
    "class SelectiveEnsemble: # once len(models) >= limit, reject new models with scores above the mean\n",
    "    def __init__(self, limit:int=None) -> None:\n",
    "        self.limit = limit \n",
    "        self.models = dict[str, IModel]()\n",
    "        self.scores = dict[str, float]()\n",
    "        self.kwargs = dict[str, dict]()\n",
    "        self.test_x, self.test_y = load_vars(test=True)\n",
    "    \n",
    "    @property\n",
    "    def mean_score(self) -> float:\n",
    "        return sum(self.scores[m] for m in self.models) / len(self) if len(self) > 0 else None\n",
    "    \n",
    "    @property\n",
    "    def best_score(self) -> float:\n",
    "        return min(self.scores[m] for m in self.models) if len(self) > 0 else None\n",
    "    \n",
    "    @property\n",
    "    def best_model(self) -> IModel:\n",
    "        return [self.models[m] for m in self.models if self.scores[m] == self.best_score][0]\n",
    "    \n",
    "    def add(self, model:IModel, name:str, kwargs:dict) -> tuple[bool, float]: # raises PredictionError\n",
    "        if name in self.models: name = f'{name}(1)'\n",
    "        pred = model.predict(self.test_x, **kwargs)\n",
    "        if len(np.unique(pred)) == 1: raise PredictionError('Model is guessing a constant value.')\n",
    "        if np.isnan(pred).any(): raise PredictionError('Model is guessing NaN.')\n",
    "        score = met.mean_absolute_error(self.test_y, pred)\n",
    "        if self.limit and len(self) >= self.limit and self.mean_score < score: return False, score\n",
    "        self.models[name] = model\n",
    "        self.scores[name] = score\n",
    "        self.kwargs[name] = kwargs\n",
    "        return True, score\n",
    "\n",
    "    def prune(self, limit:int=None) -> ext.Self: # removes models with scores above the mean; recurses if limit is set\n",
    "        pruned = SelectiveEnsemble(limit=(limit or self.limit))\n",
    "        pruned.models = {m:self.models[m] for m in self.models if self.scores[m] <= self.mean_score}\n",
    "        pruned.scores = {m:self.scores[m] for m in pruned.models}\n",
    "        pruned.kwargs = {m:self.kwargs[m] for m in pruned.models}\n",
    "        if pruned.limit and len(pruned) > pruned.limit > 1: return pruned.prune()\n",
    "        return pruned\n",
    "    \n",
    "    def clone(self, limit:int=None) -> ext.Self:\n",
    "        clone = SelectiveEnsemble(limit=(limit or self.limit))\n",
    "        clone.models = self.models.copy()\n",
    "        clone.scores = self.scores.copy()\n",
    "        clone.kwargs = self.kwargs.copy()\n",
    "        return clone\n",
    "    \n",
    "    def predict(self, X:pd.DataFrame, **kwargs) -> np.ndarray: # wrapper for soft voting; kwargs for compat\n",
    "        y = np.zeros(len(X))\n",
    "        for m in self.models:\n",
    "            pred = self.models[m].predict(X, **self.kwargs[m])\n",
    "            y += pred.reshape(-1) # reshape needed for tensorflow output; doesn't impact other model types\n",
    "        y = y / len(self)\n",
    "        return y\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.models)\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f'<SelectiveEnsemble ({len(self)} model(s); mean: {self.mean_score:.8f}; best: {self.best_score:.8f}; limit: {self.limit})>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FOLDER = '.models/'\n",
    "if not os.path.exists(MODEL_FOLDER): os.makedirs(MODEL_FOLDER)\n",
    "\n",
    "# customize fit() and predict() kwargs for each model's type and params\n",
    "def build_model_kwargs(model:IModel, val_data:tuple[pd.DataFrame, pd.Series]=None) -> tuple[dict, dict, dict]:\n",
    "    fit_kw = dict()\n",
    "    predict_kw = dict()\n",
    "    early_stop_kw = dict()\n",
    "    model_class = type(model).__name__\n",
    "    match model_class:\n",
    "        case 'Sequential':\n",
    "            model.compile(optimizer='adam', loss='mae')\n",
    "            keras_kw = dict(batch_size=256, verbose=0)\n",
    "            fit_kw.update(keras_kw)\n",
    "            predict_kw.update(keras_kw)\n",
    "            early_stop_kw['validation_data'] = val_data\n",
    "        case 'LGBMRegressor':\n",
    "            fit_kw.update(dict(verbose=False)) # verbose=0 throws an error\n",
    "            if 'early_stopping_round' in model.get_params():\n",
    "                early_stop_kw['eval_set'] = [val_data]\n",
    "                early_stop_kw['eval_metric'] = 'l1'\n",
    "        case 'XGBRegressor':\n",
    "            fit_kw.update(dict(verbose=0))\n",
    "            if 'early_stopping_rounds' in model.get_params():\n",
    "                early_stop_kw['eval_set'] = [val_data]\n",
    "        case 'CatBoostRegressor':\n",
    "            fit_kw.update(dict(verbose=0))\n",
    "            if 'early_stopping_rounds' in model.get_params():\n",
    "                early_stop_kw['eval_set'] = [val_data]\n",
    "    fit_kw.update(early_stop_kw)\n",
    "    return fit_kw, predict_kw, early_stop_kw\n",
    "\n",
    "# builds an ensemble trained on the data from load_vars(). if an existing ensemble is provided, it will be updated instead.\n",
    "def train_ensemble(models:list[IModel], folds:int=5, limit:int=None, ensemble:SelectiveEnsemble=None, skip_pred_errors:bool=True, ignore_errors:bool=True) -> SelectiveEnsemble:\n",
    "    print(f'Pre-training setup...', end='\\r')\n",
    "    ensemble = ensemble.clone(limit) if ensemble else SelectiveEnsemble(limit=limit)\n",
    "    cv = sel.TimeSeriesSplit(folds)\n",
    "    X, y = load_vars()\n",
    "    for j, model in enumerate(models):\n",
    "        model_class = type(model).__name__\n",
    "        fails = 0 # track consecutive failures\n",
    "        for i, (i_train, i_valid) in enumerate(cv.split(X)):\n",
    "            name = f'{model_class}_{int(time.time())}' # index_class_fold_timestamp\n",
    "            msg = f'Model {j+1}/{len(models)}: Fold {i+1}/{folds}: {name}'\n",
    "            print(f'{msg} - Training...', end='\\r')\n",
    "            try: # fail gracefully instead of giving up on the whole ensemble\n",
    "                X_valid, y_valid = X.iloc[i_valid, :], y.iloc[i_valid]\n",
    "                fit_kw, predict_kw, early_stop_kw = build_model_kwargs(model, (X_valid, y_valid)) \n",
    "                try: model.fit(X.iloc[i_train, :], y.iloc[i_train], **fit_kw) # some kwargs fail on kaggle\n",
    "                except: model.fit(X.iloc[i_train, :], y.iloc[i_train], **early_stop_kw) # fallback to early stop only\n",
    "                del X_valid, y_valid\n",
    "                print(f'{msg} - Submitting to ensemble...', end='\\r')\n",
    "                if model_class == 'Sequential':\n",
    "                    clone = tf.keras.models.clone_model(model)\n",
    "                    clone.set_weights(model.get_weights())\n",
    "                else: clone = None\n",
    "                res, score = ensemble.add((clone or model), name, predict_kw)\n",
    "                print(f'{msg} - {(\"Accepted\" if res else \"Rejected\")} with score: {score:.8f}\\n    {ensemble}') # spacing is intentional\n",
    "                if (res):\n",
    "                    print(f'    Saving to file...', end='\\r')\n",
    "                    save_path = os.path.join(MODEL_FOLDER, f'{name}.joblib')\n",
    "                    joblib.dump(model, save_path)\n",
    "                    print(f'    Saved to {save_path}')\n",
    "                fails = 0\n",
    "            except PredictionError as e: # these tend not to improve, so move on to next model\n",
    "                print(f'{msg} - Stopped: {e}')\n",
    "                if skip_pred_errors: break\n",
    "            except Exception as e: # these are mostly out of memory errors, which can generally be ignored\n",
    "                if not ignore_errors: raise e # ...generally\n",
    "                print(f'{msg} - Error: {type(e).__name__}: {e}')\n",
    "                fails += 1\n",
    "                if fails > 1: break # consecutive failures are usually a model misconfig, skip these as well\n",
    "            finally: gc.collect() # memory is at a premium\n",
    "    return ensemble\n",
    "\n",
    "def load_ensemble(model_dir:str=MODEL_FOLDER) -> SelectiveEnsemble:\n",
    "    ensemble = SelectiveEnsemble()\n",
    "    for file in os.listdir(model_dir):\n",
    "        model = joblib.load(os.path.join(model_dir, file))\n",
    "        name = file.split('.joblib')[0]\n",
    "        kwargs = build_model_kwargs(model, (ensemble.test_x, ensemble.test_y))[1] # only need predict_kw\n",
    "        ensemble.add(model, name, kwargs)\n",
    "    if len(ensemble) == 0: raise FileNotFoundError(f'No models saved in {model_dir}.')\n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1/7: Fold 1/5: Sequential_1702397184 - Training...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1702397188.475623   31455 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1/7: Fold 1/5: Sequential_1702397184 - Accepted with score: 5.44861698\n",
      "    <SelectiveEnsemble (1 model(s); mean: 5.44861698; best: 5.44861698; limit: 21)>\n",
      "    Saved to .models/Sequential_1702397184.joblib\n",
      "Model 1/7: Fold 2/5: Sequential_1702397195 - Accepted with score: 5.46537018\n",
      "    <SelectiveEnsemble (2 model(s); mean: 5.45699358; best: 5.44861698; limit: 21)>\n",
      "    Saved to .models/Sequential_1702397195.joblib\n",
      "Model 1/7: Fold 3/5: Sequential_1702397210 - Accepted with score: 5.44167471\n",
      "    <SelectiveEnsemble (3 model(s); mean: 5.45188729; best: 5.44167471; limit: 21)>\n",
      "    Saved to .models/Sequential_1702397210.joblib\n",
      "Model 1/7: Fold 4/5: Sequential_1702397229 - Accepted with score: 5.46247768\n",
      "    <SelectiveEnsemble (4 model(s); mean: 5.45453489; best: 5.44167471; limit: 21)>\n",
      "    Saved to .models/Sequential_1702397229.joblib\n",
      "Model 1/7: Fold 5/5: Sequential_1702397252 - Accepted with score: 5.44948196\n",
      "    <SelectiveEnsemble (5 model(s); mean: 5.45352430; best: 5.44167471; limit: 21)>\n",
      "    Saved to .models/Sequential_1702397252.joblib\n",
      "Model 2/7: Fold 1/5: Sequential_1702397282 - Accepted with score: 5.45610332\n",
      "    <SelectiveEnsemble (6 model(s); mean: 5.45395414; best: 5.44167471; limit: 21)>\n",
      "    Saved to .models/Sequential_1702397282.joblib\n",
      "Model 2/7: Fold 2/5: Sequential_1702397291 - Accepted with score: 5.47134972\n",
      "    <SelectiveEnsemble (7 model(s); mean: 5.45643922; best: 5.44167471; limit: 21)>\n",
      "    Saved to .models/Sequential_1702397291.joblib\n",
      "Model 2/7: Fold 3/5: Sequential_1702397304 - Accepted with score: 5.44910669\n",
      "    <SelectiveEnsemble (8 model(s); mean: 5.45552266; best: 5.44167471; limit: 21)>\n",
      "    Saved to .models/Sequential_1702397304.joblib\n",
      "Model 2/7: Fold 4/5: Sequential_1702397322 - Accepted with score: 5.45495176\n",
      "    <SelectiveEnsemble (9 model(s); mean: 5.45545922; best: 5.44167471; limit: 21)>\n",
      "    Saved to .models/Sequential_1702397322.joblib\n",
      "Model 2/7: Fold 5/5: Sequential_1702397345 - Accepted with score: 5.44068241\n",
      "    <SelectiveEnsemble (10 model(s); mean: 5.45398154; best: 5.44068241; limit: 21)>\n",
      "    Saved to .models/Sequential_1702397345.joblib\n",
      "Model 3/7: Fold 1/5: Sequential_1702397374 - Accepted with score: 5.45869923\n",
      "    <SelectiveEnsemble (11 model(s); mean: 5.45441042; best: 5.44068241; limit: 21)>\n",
      "    Saved to .models/Sequential_1702397374.joblib\n",
      "Model 3/7: Fold 2/5: Sequential_1702397382 - Accepted with score: 5.47260618\n",
      "    <SelectiveEnsemble (12 model(s); mean: 5.45592674; best: 5.44068241; limit: 21)>\n",
      "    Saved to .models/Sequential_1702397382.joblib\n",
      "Model 3/7: Fold 3/5: Sequential_1702397395 - Accepted with score: 5.44980812\n",
      "    <SelectiveEnsemble (13 model(s); mean: 5.45545607; best: 5.44068241; limit: 21)>\n",
      "    Saved to .models/Sequential_1702397395.joblib\n",
      "Model 3/7: Fold 4/5: Sequential_1702397414 - Accepted with score: 5.45031261\n",
      "    <SelectiveEnsemble (14 model(s); mean: 5.45508868; best: 5.44068241; limit: 21)>\n",
      "    Saved to .models/Sequential_1702397414.joblib\n",
      "Model 3/7: Fold 5/5: Sequential_1702397437 - Accepted with score: 5.45004416\n",
      "    <SelectiveEnsemble (15 model(s); mean: 5.45475238; best: 5.44068241; limit: 21)>\n",
      "    Saved to .models/Sequential_1702397437.joblib\n",
      "Model 4/7: Fold 1/5: Sequential_1702397466 - Accepted with score: 5.45807219\n",
      "    <SelectiveEnsemble (16 model(s); mean: 5.45495987; best: 5.44068241; limit: 21)>\n",
      "    Saved to .models/Sequential_1702397466.joblib\n",
      "Model 4/7: Fold 2/5: Sequential_1702397474 - Accepted with score: 5.45598030\n",
      "    <SelectiveEnsemble (17 model(s); mean: 5.45501989; best: 5.44068241; limit: 21)>\n",
      "    Saved to .models/Sequential_1702397474.joblib\n",
      "Model 4/7: Fold 3/5: Sequential_1702397488 - Accepted with score: 5.45711040\n",
      "    <SelectiveEnsemble (18 model(s); mean: 5.45513603; best: 5.44068241; limit: 21)>\n",
      "    Saved to .models/Sequential_1702397488.joblib\n",
      "Model 4/7: Fold 4/5: Sequential_1702397507 - Accepted with score: 5.45553780\n",
      "    <SelectiveEnsemble (19 model(s); mean: 5.45515718; best: 5.44068241; limit: 21)>\n",
      "    Saved to .models/Sequential_1702397507.joblib\n",
      "Model 4/7: Fold 5/5: Sequential_1702397531 - Accepted with score: 5.44994545\n",
      "    <SelectiveEnsemble (20 model(s); mean: 5.45489659; best: 5.44068241; limit: 21)>\n",
      "    Saved to .models/Sequential_1702397531.joblib\n",
      "Model 5/7: Fold 1/5: XGBRegressor_1702397560 - Accepted with score: 5.55306292\n",
      "    <SelectiveEnsemble (21 model(s); mean: 5.45957118; best: 5.44068241; limit: 21)>\n",
      "    Saved to .models/XGBRegressor_1702397560.joblib\n",
      "Model 5/7: Fold 2/5: XGBRegressor_1702397566 - Rejected with score: 5.52157068\n",
      "    <SelectiveEnsemble (21 model(s); mean: 5.45957118; best: 5.44068241; limit: 21)>\n",
      "Model 5/7: Fold 3/5: XGBRegressor_1702397576 - Rejected with score: 5.51487589\n",
      "    <SelectiveEnsemble (21 model(s); mean: 5.45957118; best: 5.44068241; limit: 21)>\n",
      "Model 5/7: Fold 4/5: XGBRegressor_1702397589 - Rejected with score: 5.49965906\n",
      "    <SelectiveEnsemble (21 model(s); mean: 5.45957118; best: 5.44068241; limit: 21)>\n",
      "Model 5/7: Fold 5/5: XGBRegressor_1702397605 - Accepted with score: 5.45572758\n",
      "    <SelectiveEnsemble (22 model(s); mean: 5.45939647; best: 5.44068241; limit: 21)>\n",
      "    Saved to .models/XGBRegressor_1702397605.joblib\n",
      "Model 6/7: Fold 1/5: LGBMRegressor_1702397618 - Rejected with score: 5.60445984\n",
      "    <SelectiveEnsemble (22 model(s); mean: 5.45939647; best: 5.44068241; limit: 21)>\n",
      "Model 6/7: Fold 2/5: LGBMRegressor_1702397622 - Rejected with score: 5.56570224\n",
      "    <SelectiveEnsemble (22 model(s); mean: 5.45939647; best: 5.44068241; limit: 21)>\n",
      "Model 6/7: Fold 3/5: LGBMRegressor_1702397629 - Rejected with score: 5.54015602\n",
      "    <SelectiveEnsemble (22 model(s); mean: 5.45939647; best: 5.44068241; limit: 21)>\n",
      "Model 6/7: Fold 4/5: LGBMRegressor_1702397640 - Rejected with score: 5.52625253\n",
      "    <SelectiveEnsemble (22 model(s); mean: 5.45939647; best: 5.44068241; limit: 21)>\n",
      "Model 6/7: Fold 5/5: LGBMRegressor_1702397654 - Rejected with score: 5.47733110\n",
      "    <SelectiveEnsemble (22 model(s); mean: 5.45939647; best: 5.44068241; limit: 21)>\n",
      "Model 7/7: Fold 1/5: CatBoostRegressor_1702397664 - Rejected with score: 5.67475530\n",
      "    <SelectiveEnsemble (22 model(s); mean: 5.45939647; best: 5.44068241; limit: 21)>\n",
      "Model 7/7: Fold 2/5: CatBoostRegressor_1702397665 - Rejected with score: 5.48738879\n",
      "    <SelectiveEnsemble (22 model(s); mean: 5.45939647; best: 5.44068241; limit: 21)>\n",
      "Model 7/7: Fold 3/5: CatBoostRegressor_1702397668 - Rejected with score: 5.47980107\n",
      "    <SelectiveEnsemble (22 model(s); mean: 5.45939647; best: 5.44068241; limit: 21)>\n",
      "Model 7/7: Fold 4/5: CatBoostRegressor_1702397671 - Rejected with score: 5.46246080\n",
      "    <SelectiveEnsemble (22 model(s); mean: 5.45939647; best: 5.44068241; limit: 21)>\n",
      "Model 7/7: Fold 5/5: CatBoostRegressor_1702397676 - Accepted with score: 5.45899006\n",
      "    <SelectiveEnsemble (23 model(s); mean: 5.45937880; best: 5.44068241; limit: 21)>\n",
      "    Saved to .models/CatBoostRegressor_1702397676.joblib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SelectiveEnsemble (23 model(s); mean: 5.45937880; best: 5.44068241; limit: 21)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_FEATURES = len(load_vars(test=True)[0].columns) # \n",
    "ACTIVATION_1 = 'tanh' # inputs are standardized so keep negative range\n",
    "ACTIVATION_2 = 'relu' # performed better than tanh, sigmoid\n",
    "DROPOUT = 0.5         # performed better than 0.3, 0.4\n",
    "RANDOM_STATE = 25     # funnier than 24\n",
    "\n",
    "layers = tf.keras.layers\n",
    "Sequential = tf.keras.Sequential\n",
    "regularizer = tf.keras.regularizers.l1(0.001)\n",
    "tf.keras.utils.set_random_seed(RANDOM_STATE)\n",
    "shared_params = dict(random_state=RANDOM_STATE, learning_rate=0.2, max_depth=3, subsample=0.8) # cat/lgb/xgb overlap\n",
    "gb_params = dict(**shared_params, n_jobs=16, colsample_bytree=0.85, reg_alpha=500) # lgb/xgb overlap\n",
    "\n",
    "models = [ # order matters; frontloading stronger models will cause more rejections if limit is set\n",
    "    Sequential([\n",
    "        layers.Dense(N_FEATURES, kernel_regularizer=regularizer, activation=ACTIVATION_1, input_shape=[N_FEATURES]),\n",
    "        layers.Dropout(DROPOUT),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(N_FEATURES//8, kernel_regularizer=regularizer, activation=ACTIVATION_2),\n",
    "        layers.Dropout(DROPOUT),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(1)\n",
    "    ]),\n",
    "    Sequential([\n",
    "        layers.Dense(N_FEATURES, kernel_regularizer=regularizer, activation=ACTIVATION_1, input_shape=[N_FEATURES]),\n",
    "        layers.Dropout(DROPOUT),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(N_FEATURES//4, kernel_regularizer=regularizer, activation=ACTIVATION_2),\n",
    "        layers.Dropout(DROPOUT),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(1)\n",
    "    ]),\n",
    "    Sequential([\n",
    "        layers.Dense(N_FEATURES, kernel_regularizer=regularizer, activation=ACTIVATION_1, input_shape=[N_FEATURES]),\n",
    "        layers.Dropout(DROPOUT),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(N_FEATURES//2, kernel_regularizer=regularizer, activation=ACTIVATION_2),\n",
    "        layers.Dropout(DROPOUT),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(1)\n",
    "    ]),\n",
    "    Sequential([\n",
    "        layers.Dense(N_FEATURES, kernel_regularizer=regularizer, activation=ACTIVATION_1, input_shape=[N_FEATURES]),\n",
    "        layers.Dropout(DROPOUT),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(N_FEATURES, kernel_regularizer=regularizer, activation=ACTIVATION_2),\n",
    "        layers.Dropout(DROPOUT),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(1)\n",
    "    ]),\n",
    "    xgb.XGBRegressor(**gb_params, early_stopping_rounds=5, eval_metric='mae', tree_method='hist', gamma=0.2),\n",
    "    lgb.LGBMRegressor(**gb_params, early_stopping_round=5, metric='l1', num_leaves=8, min_child_samples=2000, min_split_gain=0.001, verbosity=-1),\n",
    "    cat.CatBoostRegressor(**shared_params, early_stopping_rounds=5, eval_metric='MAE', num_leaves=8, min_child_samples=2000, verbose=0),\n",
    "    # svm.SVR(kernel='rbf', C=1, gamma='scale', verbose=False), # supported but slow\n",
    "]\n",
    "\n",
    "ensemble = train_ensemble(models, limit=21) if IS_TRAIN else load_ensemble()\n",
    "ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble: 5.459379, 23 models (Sequential_1702397184, Sequential_1702397195, Sequential_1702397210, Sequential_1702397229, Sequential_1702397252, Sequential_1702397282, Sequential_1702397291, Sequential_1702397304, Sequential_1702397322, Sequential_1702397345, Sequential_1702397374, Sequential_1702397382, Sequential_1702397395, Sequential_1702397414, Sequential_1702397437, Sequential_1702397466, Sequential_1702397474, Sequential_1702397488, Sequential_1702397507, Sequential_1702397531, XGBRegressor_1702397560, XGBRegressor_1702397605, CatBoostRegressor_1702397676)\n",
      "Pruned  : 5.452269, 18 models (Sequential_1702397184, Sequential_1702397210, Sequential_1702397252, Sequential_1702397282, Sequential_1702397304, Sequential_1702397322, Sequential_1702397345, Sequential_1702397374, Sequential_1702397395, Sequential_1702397414, Sequential_1702397437, Sequential_1702397466, Sequential_1702397474, Sequential_1702397488, Sequential_1702397507, Sequential_1702397531, XGBRegressor_1702397605, CatBoostRegressor_1702397676)\n",
      "Top N   : 5.447741, 9 models (Sequential_1702397184, Sequential_1702397210, Sequential_1702397252, Sequential_1702397304, Sequential_1702397345, Sequential_1702397395, Sequential_1702397414, Sequential_1702397437, Sequential_1702397531)\n"
     ]
    }
   ],
   "source": [
    "pruned = ensemble.prune()\n",
    "top = ensemble.prune(13)\n",
    "print(f'Ensemble: {ensemble.mean_score:.6f}, {len(ensemble)} models ({\", \".join([m for m in ensemble.models])})')\n",
    "print(f'Pruned  : {pruned.mean_score:.6f}, {len(pruned)} models ({\", \".join([m for m in pruned.models])})')\n",
    "print(f'Top N   : {top.mean_score:.6f}, {len(top)} models ({\", \".join([m for m in top.models])})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception # stop for manual eval\n",
    "model = ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>478_0_0</td>\n",
       "      <td>-0.637383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>478_0_1</td>\n",
       "      <td>0.523008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>478_0_2</td>\n",
       "      <td>0.788208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>478_0_3</td>\n",
       "      <td>-0.444878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>478_0_4</td>\n",
       "      <td>-0.717861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32995</th>\n",
       "      <td>480_540_195</td>\n",
       "      <td>0.165101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32996</th>\n",
       "      <td>480_540_196</td>\n",
       "      <td>-1.068835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32997</th>\n",
       "      <td>480_540_197</td>\n",
       "      <td>0.454905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32998</th>\n",
       "      <td>480_540_198</td>\n",
       "      <td>0.361118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32999</th>\n",
       "      <td>480_540_199</td>\n",
       "      <td>-0.473370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            row_id    target\n",
       "0          478_0_0 -0.637383\n",
       "1          478_0_1  0.523008\n",
       "2          478_0_2  0.788208\n",
       "3          478_0_3 -0.444878\n",
       "4          478_0_4 -0.717861\n",
       "...            ...       ...\n",
       "32995  480_540_195  0.165101\n",
       "32996  480_540_196 -1.068835\n",
       "32997  480_540_197  0.454905\n",
       "32998  480_540_198  0.361118\n",
       "32999  480_540_199 -0.473370\n",
       "\n",
       "[33000 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import optiver2023\n",
    "env = optiver2023.make_env()\n",
    "iter_test = env.iter_test()\n",
    "\n",
    "for (test, _, _) in iter_test:\n",
    "    X_test = preprocess(test)\n",
    "    y_pred = model.predict(X_test, verbose=0)\n",
    "    submission = test[['row_id']]\n",
    "    submission['target'] = y_pred\n",
    "    env.predict(submission)\n",
    "\n",
    "try:\n",
    "    res = pd.read_csv('/kaggle/working/submission.csv') # sanity check\n",
    "except FileNotFoundError:\n",
    "    res = pd.read_csv('.data/submission.csv')\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
